{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ EXTRACT WAVE 1 RESULTS - START ML NOW!\n",
    "\n",
    "## Purpose:\n",
    "Extract and consolidate Wave 1 results (12 batches Ã— 35K = 420K images) into a ready-to-use CSV for immediate ML pipeline and analysis.\n",
    "\n",
    "## Wave 1 Batches:\n",
    "- Shards 0000-0011 (12 batches)\n",
    "- 35,000 images per batch\n",
    "- Total: ~420,000 images\n",
    "- All completed successfully âœ…\n",
    "\n",
    "## Output:\n",
    "- **COMICS_OCR_WAVE1_sorted.csv** - Ready for ML training\n",
    "- Sorted by comic/page/panel\n",
    "- Full OCR data with bubbles and text\n",
    "\n",
    "## Run Time: ~5-7 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:37:53.450994Z",
     "iopub.status.busy": "2025-11-23T00:37:53.450399Z",
     "iopub.status.idle": "2025-11-23T00:37:53.565200Z",
     "shell.execute_reply": "2025-11-23T00:37:53.563616Z",
     "shell.execute_reply.started": "2025-11-23T00:37:53.450931Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "print(\"âœ… All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:38:20.050324Z",
     "iopub.status.busy": "2025-11-23T00:38:20.049447Z",
     "iopub.status.idle": "2025-11-23T00:38:20.062357Z",
     "shell.execute_reply": "2025-11-23T00:38:20.060850Z",
     "shell.execute_reply.started": "2025-11-23T00:38:20.050258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WAVE 1 EXTRACTION CONFIGURATION\n",
      "================================================================================\n",
      "Project: fluent-justice-478703-f8\n",
      "Bucket: gs://harshasekar-comics-data/\n",
      "Output prefix: ocr_outputs/optimized_35k\n",
      "Wave 1 shards: 0-11 (12 batches)\n",
      "Expected images: ~420,000\n",
      "Output CSV: COMICS_OCR_WAVE1_sorted.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "PROJECT_ID = \"fluent-justice-478703-f8\"\n",
    "LOCATION = \"us-central1\"\n",
    "BUCKET = \"harshasekar-comics-data\"\n",
    "\n",
    "# Wave 1 specific paths\n",
    "BATCH_OUTPUT_PREFIX = \"ocr_outputs/optimized_35k\"\n",
    "\n",
    "# Wave 1: Shards 0000-0011 (12 batches)\n",
    "WAVE1_SHARDS = list(range(0, 12))  # 0, 1, 2, ..., 11\n",
    "\n",
    "# Output files\n",
    "OUTPUT_DIR = Path(\".\")\n",
    "WAVE1_CSV = OUTPUT_DIR / \"COMICS_OCR_WAVE1_sorted.csv\"\n",
    "WAVE1_STATS_CSV = OUTPUT_DIR / \"COMICS_OCR_WAVE1_stats.csv\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WAVE 1 EXTRACTION CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Bucket: gs://{BUCKET}/\")\n",
    "print(f\"Output prefix: {BATCH_OUTPUT_PREFIX}\")\n",
    "print(f\"Wave 1 shards: {WAVE1_SHARDS[0]}-{WAVE1_SHARDS[-1]} ({len(WAVE1_SHARDS)} batches)\")\n",
    "print(f\"Expected images: ~{len(WAVE1_SHARDS) * 35000:,}\")\n",
    "print(f\"Output CSV: {WAVE1_CSV}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "connect-gcs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:38:30.344232Z",
     "iopub.status.busy": "2025-11-23T00:38:30.343678Z",
     "iopub.status.idle": "2025-11-23T00:38:30.402399Z",
     "shell.execute_reply": "2025-11-23T00:38:30.400436Z",
     "shell.execute_reply.started": "2025-11-23T00:38:30.344169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Connecting to Google Cloud Storage...\n",
      "âœ… Connected successfully!\n"
     ]
    }
   ],
   "source": [
    "# Connect to GCS\n",
    "print(\"ðŸ“¡ Connecting to Google Cloud Storage...\")\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.bucket(BUCKET)\n",
    "print(\"âœ… Connected successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "helper-functions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:52:41.486617Z",
     "iopub.status.busy": "2025-11-23T00:52:41.486030Z",
     "iopub.status.idle": "2025-11-23T00:52:41.505153Z",
     "shell.execute_reply": "2025-11-23T00:52:41.503560Z",
     "shell.execute_reply.started": "2025-11-23T00:52:41.486570Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_prediction_file(shard_num: int) -> str:\n",
    "    \"\"\"\n",
    "    Finds the predictions.jsonl file for a specific shard number.\n",
    "    Searches in: ocr_outputs/optimized_35k/wave1_job_XXXX/\n",
    "    \"\"\"\n",
    "    # Try wave1 path first\n",
    "    prefix = f\"{BATCH_OUTPUT_PREFIX}/wave1_job_{shard_num:04d}/\"\n",
    "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if \"prediction\" in blob.name.lower() and blob.name.endswith(\".jsonl\"):\n",
    "            return blob.name\n",
    "    \n",
    "    # Try alternative naming patterns\n",
    "    prefix = f\"{BATCH_OUTPUT_PREFIX}/job_{shard_num:04d}/\"\n",
    "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if \"prediction\" in blob.name.lower() and blob.name.endswith(\".jsonl\"):\n",
    "            return blob.name\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_gemini_ocr_line(jsonl_line: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a single JSONL line from Gemini batch output.\n",
    "    Produces clean, readable text preserving ALL original punctuation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rec = json.loads(jsonl_line)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    \n",
    "    # Extract custom_id\n",
    "    custom_id = rec.get(\"custom_id\", \"\")\n",
    "    parts = custom_id.split(\"-\")\n",
    "    \n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    \n",
    "    comic_no = parts[0]\n",
    "    page_no = parts[1]\n",
    "    panel_no = parts[2]\n",
    "    \n",
    "    img_path = f\"raw_panel_images/{comic_no}/{page_no}_{panel_no}.jpg\"\n",
    "    \n",
    "    # Extract response\n",
    "    response = rec.get(\"response\", {})\n",
    "    candidates = response.get(\"candidates\", [])\n",
    "    \n",
    "    if not candidates:\n",
    "        return {\n",
    "            \"comic_no\": comic_no,\n",
    "            \"page_no\": page_no,\n",
    "            \"panel_no\": panel_no,\n",
    "            \"img_path\": img_path,\n",
    "            \"agg_text\": \"\",\n",
    "            \"bubble_count\": 0,\n",
    "            \"bubbles_json\": json.dumps({\"bubbles\": []}),\n",
    "        }\n",
    "    \n",
    "    content = candidates[0].get(\"content\", {})\n",
    "    parts_list = content.get(\"parts\", [])\n",
    "    \n",
    "    if not parts_list:\n",
    "        return {\n",
    "            \"comic_no\": comic_no,\n",
    "            \"page_no\": page_no,\n",
    "            \"panel_no\": panel_no,\n",
    "            \"img_path\": img_path,\n",
    "            \"agg_text\": \"\",\n",
    "            \"bubble_count\": 0,\n",
    "            \"bubbles_json\": json.dumps({\"bubbles\": []}),\n",
    "        }\n",
    "    \n",
    "    raw_text = parts_list[0].get(\"text\", \"\")\n",
    "    \n",
    "    # Parse JSON response\n",
    "    bubbles_data = {\"bubbles\": []}\n",
    "    try:\n",
    "        match = re.search(r\"```(?:json)?\\s*({.*?})\\s*```\", raw_text, re.DOTALL)\n",
    "        if match:\n",
    "            bubbles_data = json.loads(match.group(1))\n",
    "        else:\n",
    "            bubbles_data = json.loads(raw_text)\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        pass\n",
    "    \n",
    "    bubbles = bubbles_data.get(\"bubbles\", [])\n",
    "    \n",
    "    # CLEAN TEXT PROCESSING - Preserves ALL punctuation\n",
    "    cleaned_texts = []\n",
    "    for bubble in bubbles:\n",
    "        text = bubble.get(\"text\", \"\")\n",
    "        \n",
    "        # Handle None\n",
    "        if text is None:\n",
    "            continue\n",
    "        \n",
    "        # Convert to string\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        # Remove literal \\n escape sequences and replace with spaces\n",
    "        text = text.replace(\"\\\\n\", \" \")\n",
    "        \n",
    "        # Replace actual newlines with spaces\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        # Replace multiple spaces with single space\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        # Remove only tab and carriage return escapes\n",
    "        text = text.replace(\"\\\\t\", \" \")\n",
    "        text = text.replace(\"\\\\r\", \" \")\n",
    "        \n",
    "        cleaned_texts.append(text)\n",
    "    \n",
    "    # Join all bubble text with space separator\n",
    "    # Preserve original punctuation - no modifications\n",
    "    agg_text = \" \".join(cleaned_texts)\n",
    "    \n",
    "    return {\n",
    "        \"comic_no\": comic_no,\n",
    "        \"page_no\": page_no,\n",
    "        \"panel_no\": panel_no,\n",
    "        \"img_path\": img_path,\n",
    "        \"agg_text\": agg_text,\n",
    "        \"bubble_count\": len(bubbles),\n",
    "        \"bubbles_json\": json.dumps(bubbles_data),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "find-outputs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:52:42.408570Z",
     "iopub.status.busy": "2025-11-23T00:52:42.408051Z",
     "iopub.status.idle": "2025-11-23T00:52:43.096081Z",
     "shell.execute_reply": "2025-11-23T00:52:43.094174Z",
     "shell.execute_reply.started": "2025-11-23T00:52:42.408525Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Finding prediction files for Wave 1 batches...\n",
      "\n",
      "âœ… Shard 0000: ocr_outputs/optimized_35k/job_0000/prediction-model-2025-11-21T11:50:37.275707Z/predictions.jsonl\n",
      "âœ… Shard 0001: ocr_outputs/optimized_35k/job_0001/prediction-model-2025-11-21T11:50:37.387656Z/predictions.jsonl\n",
      "âœ… Shard 0002: ocr_outputs/optimized_35k/job_0002/prediction-model-2025-11-21T11:50:37.494714Z/predictions.jsonl\n",
      "âœ… Shard 0003: ocr_outputs/optimized_35k/job_0003/prediction-model-2025-11-21T11:50:37.614252Z/predictions.jsonl\n",
      "âœ… Shard 0004: ocr_outputs/optimized_35k/job_0004/prediction-model-2025-11-21T11:50:37.725457Z/predictions.jsonl\n",
      "âœ… Shard 0005: ocr_outputs/optimized_35k/job_0005/prediction-model-2025-11-21T11:50:37.825309Z/predictions.jsonl\n",
      "âœ… Shard 0006: ocr_outputs/optimized_35k/job_0006/prediction-model-2025-11-21T11:50:37.941752Z/predictions.jsonl\n",
      "âœ… Shard 0007: ocr_outputs/optimized_35k/job_0007/prediction-model-2025-11-21T11:50:38.070233Z/predictions.jsonl\n",
      "âœ… Shard 0008: ocr_outputs/optimized_35k/job_0008/prediction-model-2025-11-21T11:50:38.175202Z/predictions.jsonl\n",
      "âœ… Shard 0009: ocr_outputs/optimized_35k/job_0009/prediction-model-2025-11-21T11:50:38.272607Z/predictions.jsonl\n",
      "âœ… Shard 0010: ocr_outputs/optimized_35k/job_0010/prediction-model-2025-11-21T11:50:38.454478Z/predictions.jsonl\n",
      "âœ… Shard 0011: ocr_outputs/optimized_35k/job_0011/prediction-model-2025-11-21T11:50:38.550329Z/predictions.jsonl\n",
      "\n",
      "================================================================================\n",
      "Found: 12/12 prediction files\n",
      "âœ… All Wave 1 prediction files found!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Find prediction files for Wave 1\n",
    "print(\"ðŸ” Finding prediction files for Wave 1 batches...\\n\")\n",
    "\n",
    "wave1_files = {}\n",
    "missing_shards = []\n",
    "\n",
    "for shard_num in WAVE1_SHARDS:\n",
    "    pred_file = find_prediction_file(shard_num)\n",
    "    \n",
    "    if pred_file:\n",
    "        wave1_files[shard_num] = pred_file\n",
    "        print(f\"âœ… Shard {shard_num:04d}: {pred_file}\")\n",
    "    else:\n",
    "        missing_shards.append(shard_num)\n",
    "        print(f\"âš ï¸  Shard {shard_num:04d}: No prediction file found\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Found: {len(wave1_files)}/{len(WAVE1_SHARDS)} prediction files\")\n",
    "if missing_shards:\n",
    "    print(f\"âš ï¸  Missing shards: {missing_shards}\")\n",
    "    print(f\"   Check if these batches completed successfully\")\n",
    "else:\n",
    "    print(\"âœ… All Wave 1 prediction files found!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "extract-wave1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:52:44.826071Z",
     "iopub.status.busy": "2025-11-23T00:52:44.825046Z",
     "iopub.status.idle": "2025-11-23T00:53:43.576911Z",
     "shell.execute_reply": "2025-11-23T00:53:43.575588Z",
     "shell.execute_reply.started": "2025-11-23T00:52:44.825988Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¥ Extracting OCR results from Wave 1 batches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Wave 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:58<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Extracted 420,000 panels from Wave 1\n"
     ]
    }
   ],
   "source": [
    "# Extract OCR results from Wave 1\n",
    "print(\"\\nðŸ“¥ Extracting OCR results from Wave 1 batches...\\n\")\n",
    "\n",
    "# Open CSV writer\n",
    "csv_file = WAVE1_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\")\n",
    "writer = csv.writer(csv_file)\n",
    "writer.writerow([\n",
    "    \"comic_no\", \"page_no\", \"panel_no\",\n",
    "    \"img_path\", \"agg_text\", \"bubble_count\", \"bubbles_json\"\n",
    "])\n",
    "\n",
    "# Track stats\n",
    "batch_stats = []\n",
    "total_panels = 0\n",
    "total_tokens = 0\n",
    "\n",
    "# Process each shard in order\n",
    "for shard_num in tqdm(sorted(wave1_files.keys()), desc=\"Processing Wave 1\"):\n",
    "    blob_name = wave1_files[shard_num]\n",
    "    \n",
    "    try:\n",
    "        # Download prediction file\n",
    "        blob = bucket.blob(blob_name)\n",
    "        if not blob.exists():\n",
    "            print(f\"\\nâš ï¸  Blob not found: {blob_name}\")\n",
    "            batch_stats.append({\n",
    "                \"shard_num\": shard_num,\n",
    "                \"num_panels\": 0,\n",
    "                \"total_tokens\": 0,\n",
    "                \"status\": \"blob_not_found\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        content = blob.download_as_text(encoding=\"utf-8\")\n",
    "        lines = content.strip().split(\"\\n\")\n",
    "        \n",
    "        # Parse each line\n",
    "        batch_panels = 0\n",
    "        batch_tokens = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Extract OCR data\n",
    "            parsed = parse_gemini_ocr_line(line)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            # Write to CSV\n",
    "            writer.writerow([\n",
    "                parsed[\"comic_no\"],\n",
    "                parsed[\"page_no\"],\n",
    "                parsed[\"panel_no\"],\n",
    "                parsed[\"img_path\"],\n",
    "                parsed[\"agg_text\"],\n",
    "                parsed[\"bubble_count\"],\n",
    "                parsed[\"bubbles_json\"],\n",
    "            ])\n",
    "            \n",
    "            batch_panels += 1\n",
    "            total_panels += 1\n",
    "            \n",
    "            # Track tokens\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                um = rec.get(\"response\", {}).get(\"usageMetadata\", {})\n",
    "                batch_tokens += um.get(\"totalTokenCount\", 0)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        total_tokens += batch_tokens\n",
    "        \n",
    "        batch_stats.append({\n",
    "            \"shard_num\": shard_num,\n",
    "            \"num_panels\": batch_panels,\n",
    "            \"total_tokens\": batch_tokens,\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error processing shard {shard_num}: {e}\")\n",
    "        batch_stats.append({\n",
    "            \"shard_num\": shard_num,\n",
    "            \"num_panels\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"status\": f\"error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(f\"\\nâœ… Extracted {total_panels:,} panels from Wave 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "save-stats",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:53:59.550118Z",
     "iopub.status.busy": "2025-11-23T00:53:59.549589Z",
     "iopub.status.idle": "2025-11-23T00:53:59.561433Z",
     "shell.execute_reply": "2025-11-23T00:53:59.559700Z",
     "shell.execute_reply.started": "2025-11-23T00:53:59.550069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stats saved to: COMICS_OCR_WAVE1_stats.csv\n"
     ]
    }
   ],
   "source": [
    "# Save batch statistics\n",
    "with WAVE1_STATS_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"shard_num\", \"num_panels\", \"total_tokens\", \"status\"])\n",
    "    for stat in batch_stats:\n",
    "        writer.writerow([\n",
    "            stat[\"shard_num\"],\n",
    "            stat[\"num_panels\"],\n",
    "            stat[\"total_tokens\"],\n",
    "            stat[\"status\"]\n",
    "        ])\n",
    "\n",
    "print(f\"âœ… Stats saved to: {WAVE1_STATS_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sort-csv",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:54:01.211567Z",
     "iopub.status.busy": "2025-11-23T00:54:01.210869Z",
     "iopub.status.idle": "2025-11-23T00:54:13.488186Z",
     "shell.execute_reply": "2025-11-23T00:54:13.485898Z",
     "shell.execute_reply.started": "2025-11-23T00:54:01.211474Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Sorting CSV by comic_no, page_no, panel_no...\n",
      "âœ… Sorted CSV saved: COMICS_OCR_WAVE1_sorted.csv\n",
      "   Total rows: 420,000\n"
     ]
    }
   ],
   "source": [
    "# Sort CSV by comic/page/panel for ML pipeline\n",
    "print(\"ðŸ“Š Sorting CSV by comic_no, page_no, panel_no...\")\n",
    "\n",
    "df = pd.read_csv(WAVE1_CSV)\n",
    "\n",
    "# Convert to numeric for proper sorting\n",
    "df[\"comic_no\"] = pd.to_numeric(df[\"comic_no\"], errors=\"coerce\")\n",
    "df[\"page_no\"] = pd.to_numeric(df[\"page_no\"], errors=\"coerce\")\n",
    "df[\"panel_no\"] = pd.to_numeric(df[\"panel_no\"], errors=\"coerce\")\n",
    "\n",
    "# Sort\n",
    "df = df.sort_values([\"comic_no\", \"page_no\", \"panel_no\"], ascending=[True, True, True])\n",
    "\n",
    "# Save sorted version\n",
    "df.to_csv(WAVE1_CSV, index=False)\n",
    "\n",
    "print(f\"âœ… Sorted CSV saved: {WAVE1_CSV}\")\n",
    "print(f\"   Total rows: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "summary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:48:48.152408Z",
     "iopub.status.busy": "2025-11-23T00:48:48.152005Z",
     "iopub.status.idle": "2025-11-23T00:48:48.163345Z",
     "shell.execute_reply": "2025-11-23T00:48:48.162056Z",
     "shell.execute_reply.started": "2025-11-23T00:48:48.152370Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽ‰ WAVE 1 EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Total panels extracted:     420,000\n",
      "Total tokens used:          717,192,667\n",
      "Average tokens per panel:   1707.6\n",
      "\n",
      "OUTPUT FILES:\n",
      "  ðŸ“„ Main CSV:              COMICS_OCR_WAVE1_sorted.csv\n",
      "  ðŸ“Š Stats:                 COMICS_OCR_WAVE1_stats.csv\n",
      "\n",
      "BATCH BREAKDOWN:\n",
      "  âœ… Shard 0000: 35,000 panels | 63,056,749 tokens | 1801.6 tok/panel | success\n",
      "  âœ… Shard 0001: 35,000 panels | 59,600,134 tokens | 1702.9 tok/panel | success\n",
      "  âœ… Shard 0002: 35,000 panels | 59,649,446 tokens | 1704.3 tok/panel | success\n",
      "  âœ… Shard 0003: 35,000 panels | 58,917,068 tokens | 1683.3 tok/panel | success\n",
      "  âœ… Shard 0004: 35,000 panels | 58,734,953 tokens | 1678.1 tok/panel | success\n",
      "  âœ… Shard 0005: 35,000 panels | 60,559,569 tokens | 1730.3 tok/panel | success\n",
      "  âœ… Shard 0006: 35,000 panels | 54,521,021 tokens | 1557.7 tok/panel | success\n",
      "  âœ… Shard 0007: 35,000 panels | 62,864,621 tokens | 1796.1 tok/panel | success\n",
      "  âœ… Shard 0008: 35,000 panels | 62,015,756 tokens | 1771.9 tok/panel | success\n",
      "  âœ… Shard 0009: 35,000 panels | 57,861,452 tokens | 1653.2 tok/panel | success\n",
      "  âœ… Shard 0010: 35,000 panels | 59,158,530 tokens | 1690.2 tok/panel | success\n",
      "  âœ… Shard 0011: 35,000 panels | 60,253,368 tokens | 1721.5 tok/panel | success\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… CSV is ready for ML pipeline!\n",
      "ðŸ“ˆ Start training with this dataset immediately\n",
      "ðŸ”„ Wave 2 and 3 can be merged later when they complete\n",
      "\n",
      "ðŸ’¾ Load with: df = pd.read_csv('COMICS_OCR_WAVE1_sorted.csv')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ WAVE 1 EXTRACTION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Total panels extracted:     {total_panels:,}\")\n",
    "print(f\"Total tokens used:          {total_tokens:,}\")\n",
    "print(f\"Average tokens per panel:   {total_tokens/max(total_panels, 1):.1f}\")\n",
    "print()\n",
    "print(\"OUTPUT FILES:\")\n",
    "print(f\"  ðŸ“„ Main CSV:              {WAVE1_CSV}\")\n",
    "print(f\"  ðŸ“Š Stats:                 {WAVE1_STATS_CSV}\")\n",
    "print()\n",
    "print(\"BATCH BREAKDOWN:\")\n",
    "for stat in batch_stats:\n",
    "    status_icon = \"âœ…\" if stat[\"status\"] == \"success\" else \"âš ï¸\"\n",
    "    avg_tokens = stat[\"total_tokens\"] / max(stat[\"num_panels\"], 1)\n",
    "    print(f\"  {status_icon} Shard {stat['shard_num']:04d}: {stat['num_panels']:6,} panels | \"\n",
    "          f\"{stat['total_tokens']:10,} tokens | {avg_tokens:6.1f} tok/panel | {stat['status']}\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"âœ… CSV is ready for ML pipeline!\")\n",
    "print(\"ðŸ“ˆ Start training with this dataset immediately\")\n",
    "print(\"ðŸ”„ Wave 2 and 3 can be merged later when they complete\")\n",
    "print()\n",
    "print(f\"ðŸ’¾ Load with: df = pd.read_csv('{WAVE1_CSV}')\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "preview",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:54:38.992402Z",
     "iopub.status.busy": "2025-11-23T00:54:38.991546Z",
     "iopub.status.idle": "2025-11-23T00:54:39.008399Z",
     "shell.execute_reply": "2025-11-23T00:54:39.006842Z",
     "shell.execute_reply.started": "2025-11-23T00:54:38.992357Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Data Preview:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comic_no</th>\n",
       "      <th>page_no</th>\n",
       "      <th>panel_no</th>\n",
       "      <th>img_path</th>\n",
       "      <th>agg_text</th>\n",
       "      <th>bubble_count</th>\n",
       "      <th>bubbles_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>raw_panel_images/0/0_0.jpg</td>\n",
       "      <td>JULY No.30 10c BINKS ARMORED DELIVERY MAIN ST.</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"JULY\\nNo.30\\n10c\", \"raw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>raw_panel_images/0/0_1.jpg</td>\n",
       "      <td>G ACCOUNT OF ss Wiggins MAN WHO OKE LAW OF AVI...</td>\n",
       "      <td>6</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"G ACCOUNT OF\\nss Wiggin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15839</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>raw_panel_images/0/2_0.jpg</td>\n",
       "      <td>CK HIM ,PLAS! CK HIM WN! PLASTIC MAN</td>\n",
       "      <td>2</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"CK HIM\\n,PLAS!\\nCK HIM\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24564</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>raw_panel_images/0/2_1.jpg</td>\n",
       "      <td>PLASTICMAN IN HIS CAREER AS AS CRIME-FIGHTER, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"PLASTICMAN\", \"raw_text\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14543</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>raw_panel_images/0/3_0.jpg</td>\n",
       "      <td>NOW IF I PUT THIS HERE... AND THIS ONE HERE...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"NOW IF I PUT THIS HERE....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33637</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>raw_panel_images/0/3_1.jpg</td>\n",
       "      <td>AND ADJUST THIS SPRING...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"AND ADJUST THIS SPRING....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15840</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>raw_panel_images/0/3_2.jpg</td>\n",
       "      <td>THAT DOES IT! IT'S FINISHED AND READY FOR THE ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"THAT DOES IT! IT'S FINI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>raw_panel_images/0/3_3.jpg</td>\n",
       "      <td>I'VE BEEN WORKING ON IT FOR DAYS! I MUST GET A...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"I'VE BEEN WORKING ON IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>raw_panel_images/0/3_4.jpg</td>\n",
       "      <td>AH, I FEEL BETTER ALREADY! NOW FOR A BRISK WAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"AH, I FEEL BETTER\\nALRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23050</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>raw_panel_images/0/3_5.jpg</td>\n",
       "      <td>And WEIGHTLESS WIGGINS, THUG AND PROFESSIONAL ...</td>\n",
       "      <td>2</td>\n",
       "      <td>{\"bubbles\": [{\"text\": \"And WEIGHTLESS WIGGINS,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comic_no  page_no  panel_no                    img_path  \\\n",
       "24764         0        0         0  raw_panel_images/0/0_0.jpg   \n",
       "1901          0        0         1  raw_panel_images/0/0_1.jpg   \n",
       "15839         0        2         0  raw_panel_images/0/2_0.jpg   \n",
       "24564         0        2         1  raw_panel_images/0/2_1.jpg   \n",
       "14543         0        3         0  raw_panel_images/0/3_0.jpg   \n",
       "33637         0        3         1  raw_panel_images/0/3_1.jpg   \n",
       "15840         0        3         2  raw_panel_images/0/3_2.jpg   \n",
       "4158          0        3         3  raw_panel_images/0/3_3.jpg   \n",
       "10541         0        3         4  raw_panel_images/0/3_4.jpg   \n",
       "23050         0        3         5  raw_panel_images/0/3_5.jpg   \n",
       "\n",
       "                                                agg_text  bubble_count  \\\n",
       "24764     JULY No.30 10c BINKS ARMORED DELIVERY MAIN ST.             3   \n",
       "1901   G ACCOUNT OF ss Wiggins MAN WHO OKE LAW OF AVI...             6   \n",
       "15839               CK HIM ,PLAS! CK HIM WN! PLASTIC MAN             2   \n",
       "24564  PLASTICMAN IN HIS CAREER AS AS CRIME-FIGHTER, ...             3   \n",
       "14543     NOW IF I PUT THIS HERE... AND THIS ONE HERE...             1   \n",
       "33637                          AND ADJUST THIS SPRING...             1   \n",
       "15840  THAT DOES IT! IT'S FINISHED AND READY FOR THE ...             1   \n",
       "4158   I'VE BEEN WORKING ON IT FOR DAYS! I MUST GET A...             1   \n",
       "10541  AH, I FEEL BETTER ALREADY! NOW FOR A BRISK WAL...             1   \n",
       "23050  And WEIGHTLESS WIGGINS, THUG AND PROFESSIONAL ...             2   \n",
       "\n",
       "                                            bubbles_json  \n",
       "24764  {\"bubbles\": [{\"text\": \"JULY\\nNo.30\\n10c\", \"raw...  \n",
       "1901   {\"bubbles\": [{\"text\": \"G ACCOUNT OF\\nss Wiggin...  \n",
       "15839  {\"bubbles\": [{\"text\": \"CK HIM\\n,PLAS!\\nCK HIM\\...  \n",
       "24564  {\"bubbles\": [{\"text\": \"PLASTICMAN\", \"raw_text\"...  \n",
       "14543  {\"bubbles\": [{\"text\": \"NOW IF I PUT THIS HERE....  \n",
       "33637  {\"bubbles\": [{\"text\": \"AND ADJUST THIS SPRING....  \n",
       "15840  {\"bubbles\": [{\"text\": \"THAT DOES IT! IT'S FINI...  \n",
       "4158   {\"bubbles\": [{\"text\": \"I'VE BEEN WORKING ON IT...  \n",
       "10541  {\"bubbles\": [{\"text\": \"AH, I FEEL BETTER\\nALRE...  \n",
       "23050  {\"bubbles\": [{\"text\": \"And WEIGHTLESS WIGGINS,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the data\n",
    "print(\"\\nðŸ“Š Data Preview:\\n\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for ML pipeline\n",
    "print(\"\\nðŸ“ˆ Dataset Statistics for ML:\\n\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Unique comics: {df['comic_no'].nunique():,}\")\n",
    "print(f\"\\nText Distribution:\")\n",
    "print(f\"  Panels with text: {(df['bubble_count'] > 0).sum():,} ({(df['bubble_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Empty panels: {(df['bubble_count'] == 0).sum():,} ({(df['bubble_count'] == 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average bubbles per panel: {df['bubble_count'].mean():.2f}\")\n",
    "print(f\"  Max bubbles in one panel: {df['bubble_count'].max()}\")\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = df['agg_text'].str.len()\n",
    "text_panels = df[df['text_length'] > 0]\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean: {text_panels['text_length'].mean():.1f} characters\")\n",
    "print(f\"  Median: {text_panels['text_length'].median():.1f} characters\")\n",
    "print(f\"  Max: {text_panels['text_length'].max()} characters\")\n",
    "\n",
    "# Sample size for ML\n",
    "print(f\"\\nðŸ¤– ML Training Recommendations:\")\n",
    "print(f\"  Available samples: {len(df):,}\")\n",
    "print(f\"  Suggested train/val split: 80/20\")\n",
    "print(f\"    - Training set: {int(len(df) * 0.8):,} samples\")\n",
    "print(f\"    - Validation set: {int(len(df) * 0.2):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comic distribution analysis\n",
    "print(\"\\nðŸ“š Comic Distribution (Top 20):\")\n",
    "print()\n",
    "comic_counts = df['comic_no'].value_counts().head(20)\n",
    "for comic_id, count in comic_counts.items():\n",
    "    print(f\"  Comic {int(comic_id):4d}: {count:5,} panels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "# âœ… WAVE 1 READY FOR ML!\n",
    "\n",
    "## What You Have:\n",
    "- âœ… **~420,000 images** with complete OCR data (Wave 1)\n",
    "- âœ… **Sorted by comic/page/panel** for sequential reading\n",
    "- âœ… **Full bubble data** with text, types, and structured JSON\n",
    "- âœ… **Ready for immediate ML training**\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "### 1. Start ML Pipeline NOW\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load Wave 1 data\n",
    "df = pd.read_csv('COMICS_OCR_WAVE1_sorted.csv')\n",
    "\n",
    "# Create train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Start training!\n",
    "```\n",
    "\n",
    "### 2. Sample 100 Comics for Fine-tuning\n",
    "```python\n",
    "# Get 100 random comics with most panels\n",
    "top_comics = df['comic_no'].value_counts().head(100).index.tolist()\n",
    "sample_df = df[df['comic_no'].isin(top_comics)]\n",
    "sample_df.to_csv('COMICS_OCR_SAMPLE_100.csv', index=False)\n",
    "```\n",
    "\n",
    "### 3. Continue with Wave 2 & 3 in Parallel\n",
    "- Wave 2 and 3 results can be merged later\n",
    "- Continue training with Wave 1 data\n",
    "- Final model can be updated with full 1.2M dataset\n",
    "\n",
    "## Dataset Info:\n",
    "- **Format**: CSV with structured OCR data\n",
    "- **Size**: ~420K samples (Wave 1)\n",
    "- **Features**: comic_no, page_no, panel_no, img_path, agg_text, bubble_count, bubbles_json\n",
    "- **Ready for**: LLaVA, OpenFlamingo, or custom comic OCR models\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ YOU CAN NOW START YOUR ML PIPELINE!\n",
    "\n",
    "No need to wait for Wave 2 and 3 - **start training NOW with 420K images!**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
