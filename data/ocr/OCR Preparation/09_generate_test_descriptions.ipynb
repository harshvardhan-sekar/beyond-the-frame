{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Scene Descriptions for Test Set\n",
    "\n",
    "This notebook generates Gemini scene descriptions for `test_sequences.pkl` (54,530 sequences).\n",
    "\n",
    "**Workflow:**\n",
    "1. **Part A**: Setup and load test sequences from GCS\n",
    "2. **Part B**: Create JSONL batch input files\n",
    "3. **Part C**: Upload to GCS\n",
    "4. **Part D**: Submit Gemini batch jobs (use curl in Cloud Shell)\n",
    "5. **Part E**: Check output files\n",
    "6. **Part F**: Download and parse results\n",
    "7. **Part G**: Merge and upload final file\n",
    "\n",
    "**Estimated Cost:** ~$5-6  \n",
    "**Estimated Time:** ~4-6 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART A: Setup and Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A1: Install dependencies (run once)\n",
    "!pip install google-cloud-storage --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# A2: Imports\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "from typing import List, Dict\n",
    "import math\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: fluent-justice-478703-f8\n",
      "Bucket: harshasekar-comics-data\n",
      "Model: gemini-2.5-flash-lite\n",
      "Work dir: test_batch_work\n"
     ]
    }
   ],
   "source": [
    "# A3: Configuration\n",
    "PROJECT_ID = \"fluent-justice-478703-f8\"\n",
    "BUCKET = \"harshasekar-comics-data\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Paths\n",
    "TEST_SEQUENCES_PATH = \"training_sequences/test_sequences.pkl\"\n",
    "BATCH_INPUT_PREFIX = \"batch_inputs/test_descriptions/\"\n",
    "BATCH_OUTPUT_PREFIX = \"test_descriptions/outputs/\"\n",
    "PANEL_IMAGES_PREFIX = \"raw_panel_images/\"\n",
    "\n",
    "# Batch settings\n",
    "SEQUENCES_PER_SHARD = 35000  # ~35K per shard\n",
    "MODEL = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "# Local working directory\n",
    "WORK_DIR = Path(\"./test_batch_work\")\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Bucket: {BUCKET}\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Work dir: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test sequences from GCS...\n",
      "Loaded 54,530 test sequences\n",
      "\n",
      "Sample keys: ['comic_no', 'story_idx', 'context', 'target', 'target_text', 'context_texts']\n",
      "Context panels: 5\n",
      "Target text preview: JUST GOT MY ORDERS, SWEETHEART! I'M ROCKET- TING UP TONIGHT! SEALED ORDERS-TO GET THOSE PIRATES! ONE...\n"
     ]
    }
   ],
   "source": [
    "# A4: Load test sequences from GCS\n",
    "print(\"Loading test sequences from GCS...\")\n",
    "\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket_obj = client.bucket(BUCKET)\n",
    "\n",
    "# Download test_sequences.pkl\n",
    "blob = bucket_obj.blob(TEST_SEQUENCES_PATH)\n",
    "local_pkl = WORK_DIR / \"test_sequences.pkl\"\n",
    "blob.download_to_filename(str(local_pkl))\n",
    "\n",
    "with open(local_pkl, \"rb\") as f:\n",
    "    test_sequences = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(test_sequences):,} test sequences\")\n",
    "\n",
    "# Show sample\n",
    "sample = test_sequences[0]\n",
    "print(f\"\\nSample keys: {list(sample.keys())}\")\n",
    "print(f\"Context panels: {len(sample.get('context', []))}\")\n",
    "print(f\"Target text preview: {sample.get('target_text', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART B: Create JSONL Batch Input Files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      "============================================================\n",
      "You are looking at 6 consecutive panels from a comic book.\n",
      "\n",
      "Here is the text from each panel:\n",
      "Panel 1: [No text]\n",
      "Panel 2: IN 1962 WITH A ROCKET LAND ING ON THE MOON. IN 1977, MAN SET FOOT ON MARS. A CENTURY LATER ON ALPHA CENTAURI, THE NEAREST STAF BY 3750, MANY STAR CLUSTERS HAD BEEN EXPLORED, THEIR PLANETARY SYSTEMS JOINED WITH EARTH FEDERATION. TO POLICE THIS VAST AREA OF BILLIONS OF MILES OF EMPTY SPACE-TO GUARD THE TREAS- URE-LADEN CARGO SPACERS, THE STAR PATROL WAS BORN. DAVE KENTON WAS A STAR PATROL MAN, HIS H\n",
      "Panel 3: SACK SINCE 1950, HAVE BEEN RAIDINS THE TREASURE-HEAVY SPACERS... HEAVE OVER, BOYS! WE'RE ALMOST ABOVE HER!\n",
      "Panel 4: THE SCREAMS AND MOANS OF THEIR VICTIMS SOUNDED FOR A TIME ABOVE THE WHIRR OF THE PIRATES' BEAM-GUNS- AND THEN SILENCE FELL, AND THE LOOTING BEGAN... SURRENDER NOW- AND YOU LIVE! FIGHT- AND DIE!\n",
      "Panel 5: ON THE TINY PLANET OF FLAYAL-HUNDREDS OF LIGHT YEARS FROM THE EARTH-YOUNG STAR PATROLMAN DAVE KENTON RECEIVES WORD OF THE SPACE DISASTER... THE COALSACK PIRATES-AGAIN! GOT THE ANNUAL METAL RUN FROM DEBEB! THREE BILLIONS CREDITS WORTH OF GOLD AND PLATINUM- GONE!\n",
      "Panel 6: JUST GOT MY ORDERS, SWEETHEART! I'M ROCKET- TING UP TONIGHT! SEALED ORDERS-TO GET THOSE PIRATES! ONE MAN- AGAINST A FLEET OF SPACE ROVERS?\n",
      "\n",
      "Based on what you see in these panels, describe what happens in Panel 6 (the last panel).\n",
      "\n",
      "Include the scene, any dialogue, and sound effects.\n",
      "\n",
      "Write your response as a single flowing paragraph. Do not use bullet points, numbered lists, bold text, asterisks, or any markdown formatting. Weave the dialogue naturally into your description.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# B1: Define the prompt template (same as training)\n",
    "\n",
    "def create_prompt(context_texts: List[str], target_text: str) -> str:\n",
    "    \"\"\"Create the prompt for Gemini - matches LLaVA fine-tuning format.\"\"\"\n",
    "    prompt = \"\"\"You are looking at 6 consecutive panels from a comic book.\n",
    "\n",
    "Here is the text from each panel:\n",
    "\"\"\"\n",
    "    # Add context panels (1-5)\n",
    "    for i, text in enumerate(context_texts[-5:], 1):  # Last 5 context panels\n",
    "        if text and text.strip():\n",
    "            prompt += f\"Panel {i}: {text.strip()[:400]}\\n\"\n",
    "        else:\n",
    "            prompt += f\"Panel {i}: [No text]\\n\"\n",
    "    \n",
    "    # Add target panel (6)\n",
    "    if target_text and target_text.strip():\n",
    "        prompt += f\"Panel 6: {target_text.strip()[:400]}\\n\"\n",
    "    else:\n",
    "        prompt += f\"Panel 6: [No text]\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "Based on what you see in these panels, describe what happens in Panel 6 (the last panel).\n",
    "\n",
    "Include the scene, any dialogue, and sound effects.\n",
    "\n",
    "Write your response as a single flowing paragraph. Do not use bullet points, numbered lists, bold text, asterisks, or any markdown formatting. Weave the dialogue naturally into your description.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test prompt\n",
    "sample_prompt = create_prompt(\n",
    "    test_sequences[0].get(\"context_texts\", []),\n",
    "    test_sequences[0].get(\"target_text\", \"\")\n",
    ")\n",
    "print(\"Sample prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(sample_prompt)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample request structure:\n",
      "  customId: test_0_comic1\n",
      "  Number of image parts: 6\n",
      "  First image URI: gs://harshasekar-comics-data/raw_panel_images/47_0.jpg...\n"
     ]
    }
   ],
   "source": [
    "# B2: Create batch request for a single sequence\n",
    "\n",
    "def create_batch_request(seq: Dict, seq_idx: int) -> Dict:\n",
    "    \"\"\"Create a single batch request with 6 images + prompt.\"\"\"\n",
    "    \n",
    "    # Get image URIs for context panels (last 5)\n",
    "    context_panels = seq.get(\"context\", [])[-5:]\n",
    "    target_panel = seq.get(\"target\", {})\n",
    "    \n",
    "    # Build image parts\n",
    "    image_parts = []\n",
    "    \n",
    "    # Add 5 context panel images\n",
    "    for panel in context_panels:\n",
    "        img_path = panel.get(\"image_path\", \"\")\n",
    "        # Convert local path to GCS URI\n",
    "        if \"/raw_panel_images/\" in img_path:\n",
    "            gcs_path = img_path.split(\"/raw_panel_images/\")[-1]\n",
    "        else:\n",
    "            gcs_path = img_path.split(\"/\")[-1] if \"/\" in img_path else img_path\n",
    "        \n",
    "        gcs_uri = f\"gs://{BUCKET}/{PANEL_IMAGES_PREFIX}{gcs_path}\"\n",
    "        image_parts.append({\n",
    "            \"fileData\": {\n",
    "                \"mimeType\": \"image/jpeg\",\n",
    "                \"fileUri\": gcs_uri\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Add target panel image (panel 6)\n",
    "    target_img_path = target_panel.get(\"image_path\", \"\")\n",
    "    if \"/raw_panel_images/\" in target_img_path:\n",
    "        gcs_path = target_img_path.split(\"/raw_panel_images/\")[-1]\n",
    "    else:\n",
    "        gcs_path = target_img_path.split(\"/\")[-1] if \"/\" in target_img_path else target_img_path\n",
    "    \n",
    "    gcs_uri = f\"gs://{BUCKET}/{PANEL_IMAGES_PREFIX}{gcs_path}\"\n",
    "    image_parts.append({\n",
    "        \"fileData\": {\n",
    "            \"mimeType\": \"image/jpeg\",\n",
    "            \"fileUri\": gcs_uri\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Create prompt\n",
    "    context_texts = seq.get(\"context_texts\", [])\n",
    "    target_text = seq.get(\"target_text\", \"\")\n",
    "    prompt = create_prompt(context_texts, target_text)\n",
    "    \n",
    "    # Build the request\n",
    "    request = {\n",
    "        \"request\": {\n",
    "            \"contents\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": image_parts + [{\"text\": prompt}]\n",
    "                }\n",
    "            ],\n",
    "            \"generationConfig\": {\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_output_tokens\": 512,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add custom ID for tracking\n",
    "    comic_no = seq.get(\"comic_no\", 0)\n",
    "    request[\"customId\"] = f\"test_{seq_idx}_comic{comic_no}\"\n",
    "    \n",
    "    return request\n",
    "\n",
    "# Test with first sequence\n",
    "test_request = create_batch_request(test_sequences[0], 0)\n",
    "print(\"Sample request structure:\")\n",
    "print(f\"  customId: {test_request['customId']}\")\n",
    "print(f\"  Number of image parts: {len(test_request['request']['contents'][0]['parts']) - 1}\")\n",
    "print(f\"  First image URI: {test_request['request']['contents'][0]['parts'][0]['fileData']['fileUri'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B3: Create sharded JSONL files\n",
    "\n",
    "num_sequences = len(test_sequences)\n",
    "num_shards = math.ceil(num_sequences / SEQUENCES_PER_SHARD)\n",
    "\n",
    "print(f\"Total sequences: {num_sequences:,}\")\n",
    "print(f\"Sequences per shard: {SEQUENCES_PER_SHARD:,}\")\n",
    "print(f\"Number of shards: {num_shards}\")\n",
    "print()\n",
    "\n",
    "shard_files = []\n",
    "\n",
    "for shard_idx in range(num_shards):\n",
    "    start_idx = shard_idx * SEQUENCES_PER_SHARD\n",
    "    end_idx = min(start_idx + SEQUENCES_PER_SHARD, num_sequences)\n",
    "    \n",
    "    shard_file = WORK_DIR / f\"test_shard_{shard_idx:04d}.jsonl\"\n",
    "    shard_files.append(shard_file)\n",
    "    \n",
    "    with open(shard_file, \"w\") as f:\n",
    "        for seq_idx in range(start_idx, end_idx):\n",
    "            seq = test_sequences[seq_idx]\n",
    "            request = create_batch_request(seq, seq_idx)\n",
    "            f.write(json.dumps(request) + \"\\n\")\n",
    "    \n",
    "    file_size_mb = shard_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Shard {shard_idx}: {end_idx - start_idx:,} sequences, {file_size_mb:.1f} MB ‚Üí {shard_file.name}\")\n",
    "\n",
    "print(f\"\\nCreated {len(shard_files)} shard files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART C: Upload JSONL Files to GCS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1: Upload shard files to GCS\n",
    "\n",
    "print(\"Uploading shard files to GCS...\")\n",
    "print()\n",
    "\n",
    "gcs_shard_uris = []\n",
    "\n",
    "for shard_file in shard_files:\n",
    "    gcs_path = f\"{BATCH_INPUT_PREFIX}{shard_file.name}\"\n",
    "    blob = bucket_obj.blob(gcs_path)\n",
    "    blob.upload_from_filename(str(shard_file))\n",
    "    \n",
    "    gcs_uri = f\"gs://{BUCKET}/{gcs_path}\"\n",
    "    gcs_shard_uris.append(gcs_uri)\n",
    "    print(f\"Uploaded: {gcs_uri}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Uploaded {len(gcs_shard_uris)} shard files to GCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART D: Submit Batch Jobs\n",
    "---\n",
    "\n",
    "**Run these curl commands in Google Cloud Shell (not in this notebook).**\n",
    "\n",
    "The cell below generates the commands for you to copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1: Generate curl commands for batch submission\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CURL COMMANDS FOR CLOUD SHELL\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Copy and paste these commands into Google Cloud Shell:\")\n",
    "print()\n",
    "\n",
    "for i, gcs_uri in enumerate(gcs_shard_uris):\n",
    "    output_uri = f\"gs://{BUCKET}/{BATCH_OUTPUT_PREFIX}job_{i:04d}/\"\n",
    "    \n",
    "    curl_cmd = f'''# Batch {i}\n",
    "curl -X POST \\\\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs \\\\\n",
    "  -d '{{\n",
    "    \"displayName\": \"test-desc-batch-{i}\",\n",
    "    \"model\": \"publishers/google/models/{MODEL}\",\n",
    "    \"inputConfig\": {{\n",
    "      \"instancesFormat\": \"jsonl\",\n",
    "      \"gcsSource\": {{\"uris\": [\"{gcs_uri}\"]}}\n",
    "    }},\n",
    "    \"outputConfig\": {{\n",
    "      \"predictionsFormat\": \"jsonl\",\n",
    "      \"gcsDestination\": {{\"outputUriPrefix\": \"{output_uri}\"}}\n",
    "    }}\n",
    "  }}'\n",
    "'''\n",
    "    print(curl_cmd)\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MONITORING COMMAND\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f'''curl -X GET \\\\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\\\n",
    "  \"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/batchPredictionJobs\" | python3 -m json.tool''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART E: Check Output Files (After Jobs Complete)\n",
    "---\n",
    "\n",
    "**Wait for all batch jobs to complete before running this section.**\n",
    "\n",
    "Check job status with the monitoring command above. Look for `\"state\": \"JOB_STATE_SUCCEEDED\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E1: List output files in GCS\n",
    "\n",
    "print(\"Checking output files in GCS...\")\n",
    "print()\n",
    "\n",
    "output_blobs = list(bucket_obj.list_blobs(prefix=BATCH_OUTPUT_PREFIX))\n",
    "jsonl_files = [b for b in output_blobs if b.name.endswith('.jsonl')]\n",
    "\n",
    "print(f\"Found {len(jsonl_files)} JSONL output files:\")\n",
    "print()\n",
    "\n",
    "total_size = 0\n",
    "for blob in jsonl_files:\n",
    "    size_mb = blob.size / (1024 * 1024)\n",
    "    total_size += blob.size\n",
    "    print(f\"  {blob.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal output size: {total_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART F: Download and Parse Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1: Download and parse all results\n",
    "\n",
    "print(\"Downloading and parsing results...\")\n",
    "print()\n",
    "\n",
    "results = {}  # customId -> scene_description\n",
    "errors = []\n",
    "\n",
    "for blob in jsonl_files:\n",
    "    print(f\"Processing: {blob.name}\")\n",
    "    \n",
    "    # Download content\n",
    "    content = blob.download_as_text()\n",
    "    \n",
    "    for line in content.strip().split(\"\\n\"):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            custom_id = data.get(\"customId\", \"\")\n",
    "            \n",
    "            # Extract the response text\n",
    "            response = data.get(\"response\", {})\n",
    "            candidates = response.get(\"candidates\", [])\n",
    "            \n",
    "            if candidates:\n",
    "                content_parts = candidates[0].get(\"content\", {}).get(\"parts\", [])\n",
    "                if content_parts:\n",
    "                    scene_desc = content_parts[0].get(\"text\", \"\")\n",
    "                    results[custom_id] = scene_desc.strip()\n",
    "            else:\n",
    "                errors.append({\"customId\": custom_id, \"error\": \"No candidates\"})\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors.append({\"line\": line[:100], \"error\": str(e)})\n",
    "\n",
    "print()\n",
    "print(f\"‚úÖ Parsed {len(results):,} scene descriptions\")\n",
    "print(f\"‚ùå Errors: {len(errors)}\")\n",
    "\n",
    "if errors[:3]:\n",
    "    print(\"\\nSample errors:\")\n",
    "    for e in errors[:3]:\n",
    "        print(f\"  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2: Show sample results\n",
    "\n",
    "print(\"Sample scene descriptions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_ids = list(results.keys())[:3]\n",
    "for custom_id in sample_ids:\n",
    "    desc = results[custom_id]\n",
    "    print(f\"\\n{custom_id}:\")\n",
    "    print(f\"  {desc[:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART G: Merge and Upload Final File\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G1: Merge scene descriptions into test sequences\n",
    "\n",
    "print(\"Merging scene descriptions into test sequences...\")\n",
    "\n",
    "merged_count = 0\n",
    "missing_count = 0\n",
    "\n",
    "for seq_idx, seq in enumerate(test_sequences):\n",
    "    comic_no = seq.get(\"comic_no\", 0)\n",
    "    custom_id = f\"test_{seq_idx}_comic{comic_no}\"\n",
    "    \n",
    "    if custom_id in results:\n",
    "        seq[\"scene_description\"] = results[custom_id]\n",
    "        merged_count += 1\n",
    "    else:\n",
    "        seq[\"scene_description\"] = \"\"  # Empty for missing\n",
    "        missing_count += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Merged: {merged_count:,}\")\n",
    "print(f\"‚ö†Ô∏è  Missing: {missing_count:,}\")\n",
    "print(f\"üìä Coverage: {merged_count/len(test_sequences)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G2: Save locally and upload to GCS\n",
    "\n",
    "# Save locally\n",
    "local_output = WORK_DIR / \"test_sequences_with_descriptions.pkl\"\n",
    "with open(local_output, \"wb\") as f:\n",
    "    pickle.dump(test_sequences, f)\n",
    "\n",
    "file_size_mb = local_output.stat().st_size / (1024 * 1024)\n",
    "print(f\"Saved locally: {local_output} ({file_size_mb:.1f} MB)\")\n",
    "\n",
    "# Upload to GCS\n",
    "gcs_output_path = \"training_sequences/test_sequences_with_descriptions.pkl\"\n",
    "blob = bucket_obj.blob(gcs_output_path)\n",
    "blob.upload_from_filename(str(local_output))\n",
    "\n",
    "print(f\"\\n‚úÖ Uploaded to: gs://{BUCKET}/{gcs_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G3: Show comparison - OCR vs Scene Description\n",
    "\n",
    "print(\"Comparison: OCR Text vs Scene Description\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(3):\n",
    "    seq = test_sequences[i]\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"\\nüìù OCR (target_text):\")\n",
    "    print(f\"   {seq.get('target_text', '[Empty]')[:200]}\")\n",
    "    print(f\"\\nüé¨ Scene Description:\")\n",
    "    print(f\"   {seq.get('scene_description', '[Empty]')[:300]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "---\n",
    "\n",
    "## Files Created\n",
    "\n",
    "```\n",
    "gs://harshasekar-comics-data/training_sequences/\n",
    "‚îú‚îÄ‚îÄ test_sequences.pkl                      ‚Üê Original\n",
    "‚îî‚îÄ‚îÄ test_sequences_with_descriptions.pkl    ‚Üê NEW (with Gemini descriptions)\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Copy to Delta:**\n",
    "   ```bash\n",
    "   gsutil cp gs://harshasekar-comics-data/training_sequences/test_sequences_with_descriptions.pkl \\\n",
    "     /scratch/bftl/hsekar/comics_project/data/processed/\n",
    "   ```\n",
    "\n",
    "2. **Update fine-tuning notebook** with the 5 changes identified earlier\n",
    "\n",
    "3. **Re-run fine-tuning** with proper scene description targets"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
