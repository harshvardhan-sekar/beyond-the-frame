{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Comics OCR Pipeline - Final Optimized Version\n",
    "\n",
    "## Key Features:\n",
    "- ‚úÖ **Filters macOS metadata files** (`._` files)\n",
    "- ‚úÖ **35K batch size** (19-20 hour completion, 0% timeout)\n",
    "- ‚úÖ **Natural sorting** (0, 1, 2, 3... not 0, 1, 10, 100...)\n",
    "- ‚úÖ **Wave submission** (12 batches at a time)\n",
    "- ‚úÖ **Automatic CSV merge** (sorted by comic/page/panel)\n",
    "\n",
    "## Expected Results:\n",
    "- ~1.2M valid images (not 2.4M!)\n",
    "- 35 shards √ó 35K images each\n",
    "- 3 waves of submissions\n",
    "- ~60 hours total processing time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "# üìã STEP 1: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:34:30.017529Z",
     "iopub.status.busy": "2025-11-21T11:34:30.017167Z",
     "iopub.status.idle": "2025-11-21T11:34:30.058605Z",
     "shell.execute_reply": "2025-11-21T11:34:30.057361Z",
     "shell.execute_reply.started": "2025-11-21T11:34:30.017496Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:34:31.683557Z",
     "iopub.status.busy": "2025-11-21T11:34:31.683086Z",
     "iopub.status.idle": "2025-11-21T11:34:31.693714Z",
     "shell.execute_reply": "2025-11-21T11:34:31.692553Z",
     "shell.execute_reply.started": "2025-11-21T11:34:31.683517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      "Project: fluent-justice-478703-f8\n",
      "Bucket: gs://harshasekar-comics-data/\n",
      "Batch size: 35,000 images per shard\n",
      "Expected time per batch: 19-20 hours\n",
      "Timeout risk: 0% ‚úÖ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "PROJECT_ID = \"fluent-justice-478703-f8\"\n",
    "LOCATION   = \"us-central1\"\n",
    "BUCKET     = \"harshasekar-comics-data\"\n",
    "\n",
    "IMAGES_PREFIX = \"raw_panel_images\"\n",
    "\n",
    "# ‚úÖ OPTIMIZED: 35K batch size (guaranteed completion in 19-20 hours)\n",
    "SHARD_SIZE = 35000\n",
    "\n",
    "BATCH_INPUT_PREFIX  = \"batch_inputs/optimized_35k\"\n",
    "BATCH_OUTPUT_PREFIX = \"ocr_outputs/optimized_35k\"\n",
    "\n",
    "# Local paths\n",
    "WORKDIR = Path(\".\")\n",
    "SHARDS_DIR = WORKDIR / \"jsonl_shards_35k\"\n",
    "PREDAPAGES_PATH = WORKDIR / \"predadpages.txt\"\n",
    "\n",
    "# Create directories\n",
    "SHARDS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Bucket: gs://{BUCKET}/\")\n",
    "print(f\"Batch size: {SHARD_SIZE:,} images per shard\")\n",
    "print(f\"Expected time per batch: 19-20 hours\")\n",
    "print(f\"Timeout risk: 0% ‚úÖ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gcs-client",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:34:34.628605Z",
     "iopub.status.busy": "2025-11-21T11:34:34.628049Z",
     "iopub.status.idle": "2025-11-21T11:34:34.650955Z",
     "shell.execute_reply": "2025-11-21T11:34:34.649568Z",
     "shell.execute_reply.started": "2025-11-21T11:34:34.628558Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Connecting to Google Cloud Storage...\n",
      "‚úÖ Connected to GCS successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google Cloud Storage client\n",
    "print(\"üì° Connecting to Google Cloud Storage...\")\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.bucket(BUCKET)\n",
    "print(\"‚úÖ Connected to GCS successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "# üìÑ STEP 2: Load Ad Pages to Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-ad-pages",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:34:37.666358Z",
     "iopub.status.busy": "2025-11-21T11:34:37.665970Z",
     "iopub.status.idle": "2025-11-21T11:34:37.692318Z",
     "shell.execute_reply": "2025-11-21T11:34:37.691172Z",
     "shell.execute_reply.started": "2025-11-21T11:34:37.666319Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 13,200 ad pages to skip\n"
     ]
    }
   ],
   "source": [
    "def load_ad_pages(path: Path):\n",
    "    \"\"\"\n",
    "    Load ad pages from predadpages.txt\n",
    "    Format: comic_id---page_number\n",
    "    \"\"\"\n",
    "    ad_pages = set()\n",
    "    \n",
    "    if not path.exists():\n",
    "        print(\"‚ö†Ô∏è  predadpages.txt not found - no ad pages will be filtered\")\n",
    "        return ad_pages\n",
    "    \n",
    "    with path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.split(\"---\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            comic, page = parts\n",
    "            ad_pages.add((comic.strip(), page.strip()))\n",
    "    \n",
    "    return ad_pages\n",
    "\n",
    "# Load ad pages\n",
    "ad_pages = load_ad_pages(PREDAPAGES_PATH)\n",
    "print(f\"‚úÖ Loaded {len(ad_pages):,} ad pages to skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "# üî¢ STEP 3: Natural Sorting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "natural-sort",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:34:38.678576Z",
     "iopub.status.busy": "2025-11-21T11:34:38.677548Z",
     "iopub.status.idle": "2025-11-21T11:34:38.686970Z",
     "shell.execute_reply": "2025-11-21T11:34:38.685748Z",
     "shell.execute_reply.started": "2025-11-21T11:34:38.678530Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural sort test:\n",
      "  Before: ['0', '1', '10', '100', '1000', '2', '20', '200']\n",
      "  After:  ['0', '1', '2', '10', '20', '100', '200', '1000']\n",
      "  ‚úÖ Sorting works correctly!\n"
     ]
    }
   ],
   "source": [
    "def natural_sort_key(s):\n",
    "    \"\"\"\n",
    "    Converts string to sortable format for natural sorting.\n",
    "    Example: Sorts as 0, 1, 2, 3, ... 10, 11, ... 100, 101\n",
    "    Instead of: 0, 1, 10, 100, 1000, 1001, ... 2, 20, 200\n",
    "    \"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() \n",
    "            for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "# Test the function\n",
    "test_comics = ['0', '1', '10', '100', '1000', '2', '20', '200']\n",
    "sorted_test = sorted(test_comics, key=natural_sort_key)\n",
    "\n",
    "print(\"Natural sort test:\")\n",
    "print(f\"  Before: {test_comics}\")\n",
    "print(f\"  After:  {sorted_test}\")\n",
    "print(\"  ‚úÖ Sorting works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "# üìù STEP 4: OCR Request Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ocr-prompt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:39:33.865640Z",
     "iopub.status.busy": "2025-11-21T11:39:33.865237Z",
     "iopub.status.idle": "2025-11-21T11:39:33.874399Z",
     "shell.execute_reply": "2025-11-21T11:39:33.873290Z",
     "shell.execute_reply.started": "2025-11-21T11:39:33.865605Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OCR request builder configured\n"
     ]
    }
   ],
   "source": [
    "OCR_SYSTEM_PROMPT = \"\"\"\n",
    "You are an OCR extractor for comic book panels.\n",
    "Your job is to read ONLY the text actually visible in the image.\n",
    "Do NOT guess, rewrite, translate, or hallucinate new text.\n",
    "\n",
    "Return output in this exact JSON format:\n",
    "\n",
    "{\n",
    "  \"bubbles\": [\n",
    "    {\n",
    "      \"text\": \"<cleaned text>\",\n",
    "      \"raw_text\": \"<as seen>\",\n",
    "      \"type\": \"dialogue | narration | sound_effect | sign | unknown\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Extract all text from speech bubbles, narration boxes, signs, and sound effects.\n",
    "- Clean simple OCR noise but do not change meaning.\n",
    "- If text is unclear, transcribe as faithfully as possible.\n",
    "- If the panel contains no text, return: {\"bubbles\": []}\n",
    "- Never add or imagine text that is not clearly visible.\n",
    "\"\"\"\n",
    "\n",
    "def make_request_line(image_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a batch request for one image.\n",
    "    image_path: 'raw_panel_images/{comic_id}/{page}_{panel}.jpg'\n",
    "    Returns: {\"custom_id\": \"...\", \"request\": {...}}\n",
    "    \"\"\"\n",
    "    _, comic_id, fname = image_path.split(\"/\", 2)\n",
    "    page_str, panel_str = fname.replace(\".jpg\", \"\").split(\"_\", 1)\n",
    "\n",
    "    file_uri = f\"gs://{BUCKET}/{image_path}\"\n",
    "\n",
    "    request_body = {\n",
    "        \"system_instruction\": {\n",
    "            \"role\": \"system\",\n",
    "            \"parts\": [{\"text\": OCR_SYSTEM_PROMPT}],\n",
    "        },\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"file_data\": {\n",
    "                            \"file_uri\": file_uri,\n",
    "                            \"mime_type\": \"image/jpeg\",\n",
    "                        }\n",
    "                    },\n",
    "                    {\"text\": \"Extract OCR text from this panel.\"},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"generation_config\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_output_tokens\": 700,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"custom_id\": f\"{comic_id}-{page_str}-{panel_str}\",\n",
    "        \"request\": request_body,\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ OCR request builder configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "# üîç STEP 5: Scan & Filter Images (WITH METADATA FILTER!)\n",
    "\n",
    "## This is the critical step that fixes the 2.4M ‚Üí 1.2M issue!\n",
    "\n",
    "Filters applied:\n",
    "1. ‚úÖ **macOS metadata files** (`._filename.jpg`)\n",
    "2. ‚úÖ **Hidden files** (starting with `.`)\n",
    "3. ‚úÖ **Ad pages** (from predadpages.txt)\n",
    "4. ‚úÖ **Invalid formats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "scan-images",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:34:45.504543Z",
     "iopub.status.busy": "2025-11-21T11:34:45.503839Z",
     "iopub.status.idle": "2025-11-21T11:37:46.486856Z",
     "shell.execute_reply": "2025-11-21T11:37:46.485374Z",
     "shell.execute_reply.started": "2025-11-21T11:34:45.504506Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Scanning all panel images from Cloud Storage...\n",
      "   Path: gs://harshasekar-comics-data/raw_panel_images/\n",
      "   This may take a few minutes...\n",
      "\n",
      "‚úÖ Found 2,463,260 total files (includes metadata)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÇ Scanning all panel images from Cloud Storage...\")\n",
    "print(f\"   Path: gs://{BUCKET}/{IMAGES_PREFIX}/\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "# List all blobs\n",
    "blobs = list(bucket.list_blobs(prefix=IMAGES_PREFIX))\n",
    "print(f\"‚úÖ Found {len(blobs):,} total files (includes metadata)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "filter-images",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:37:52.541946Z",
     "iopub.status.busy": "2025-11-21T11:37:52.541102Z",
     "iopub.status.idle": "2025-11-21T11:37:57.376676Z",
     "shell.execute_reply": "2025-11-21T11:37:57.375566Z",
     "shell.execute_reply.started": "2025-11-21T11:37:52.541906Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Filtering images...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Organizing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2463260/2463260 [00:04<00:00, 512791.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FILTERING RESULTS\n",
      "================================================================================\n",
      "Total files scanned:           2,463,260\n",
      "\n",
      "Skipped:\n",
      "  ‚Ä¢ macOS metadata (._*):      1,229,664\n",
      "  ‚Ä¢ Ad pages:                  49,821\n",
      "  ‚Ä¢ Invalid format:            1\n",
      "  ‚Ä¢ Non-JPG files:             3,931\n",
      "  TOTAL SKIPPED:               1,283,417\n",
      "\n",
      "Valid images:\n",
      "  ‚Ä¢ Unique comics:             3,929\n",
      "  ‚Ä¢ Total valid panels:        1,179,843\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Found 1,179,843 valid images - looks correct!\n",
      "   (Expected ~1.2M, got 1.18M)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Organize by comic WITH PROPER FILTERING\n",
    "comics_data = defaultdict(list)\n",
    "\n",
    "# Tracking counters\n",
    "skipped_metadata = 0\n",
    "skipped_ad_pages = 0\n",
    "skipped_invalid = 0\n",
    "skipped_not_jpg = 0\n",
    "\n",
    "print(\"üîç Filtering images...\\n\")\n",
    "\n",
    "for blob in tqdm(blobs, desc=\"Organizing images\"):\n",
    "    # Filter 1: Must end with .jpg\n",
    "    if not blob.name.endswith(\".jpg\"):\n",
    "        skipped_not_jpg += 1\n",
    "        continue\n",
    "    \n",
    "    parts = blob.name.split(\"/\")\n",
    "    if len(parts) < 3:\n",
    "        skipped_invalid += 1\n",
    "        continue\n",
    "    \n",
    "    comic_id = parts[1]\n",
    "    fname = parts[2]\n",
    "    \n",
    "    # Filter 2: Skip macOS metadata files (._filename.jpg)\n",
    "    if fname.startswith(\"._\"):\n",
    "        skipped_metadata += 1\n",
    "        continue\n",
    "    \n",
    "    # Filter 3: Skip any other hidden files\n",
    "    if fname.startswith(\".\"):\n",
    "        skipped_metadata += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        page_str, panel_str = fname.replace(\".jpg\", \"\").split(\"_\", 1)\n",
    "    except ValueError:\n",
    "        skipped_invalid += 1\n",
    "        continue\n",
    "    \n",
    "    # Filter 4: Skip ad pages\n",
    "    if (comic_id, page_str) in ad_pages:\n",
    "        skipped_ad_pages += 1\n",
    "        continue\n",
    "    \n",
    "    # Parse page and panel numbers\n",
    "    try:\n",
    "        page_num = int(page_str)\n",
    "        panel_num = int(panel_str)\n",
    "    except ValueError:\n",
    "        skipped_invalid += 1\n",
    "        continue\n",
    "    \n",
    "    # Valid image! Add to dataset\n",
    "    comics_data[comic_id].append((page_num, panel_num, blob.name))\n",
    "\n",
    "# Calculate totals\n",
    "total_valid = sum(len(panels) for panels in comics_data.values())\n",
    "total_skipped = skipped_metadata + skipped_ad_pages + skipped_invalid + skipped_not_jpg\n",
    "\n",
    "# Print filtering results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FILTERING RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total files scanned:           {len(blobs):,}\")\n",
    "print(f\"\\nSkipped:\")\n",
    "print(f\"  ‚Ä¢ macOS metadata (._*):      {skipped_metadata:,}\")\n",
    "print(f\"  ‚Ä¢ Ad pages:                  {skipped_ad_pages:,}\")\n",
    "print(f\"  ‚Ä¢ Invalid format:            {skipped_invalid:,}\")\n",
    "print(f\"  ‚Ä¢ Non-JPG files:             {skipped_not_jpg:,}\")\n",
    "print(f\"  TOTAL SKIPPED:               {total_skipped:,}\")\n",
    "print(f\"\\nValid images:\")\n",
    "print(f\"  ‚Ä¢ Unique comics:             {len(comics_data):,}\")\n",
    "print(f\"  ‚Ä¢ Total valid panels:        {total_valid:,}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Verify we have the right amount\n",
    "if total_valid < 1_000_000:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Only found {total_valid:,} images (expected ~1.2M)\")\n",
    "    print(\"   Check if filtering is too aggressive\")\n",
    "elif total_valid > 1_500_000:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Found {total_valid:,} images (expected ~1.2M)\")\n",
    "    print(\"   Some metadata files may not have been filtered\")\n",
    "    print(\"   Check for other hidden file patterns\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {total_valid:,} valid images - looks correct!\")\n",
    "    print(f\"   (Expected ~1.2M, got {total_valid/1_000_000:.2f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "# üî® STEP 6: Create 35K Shards with Natural Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sort-comics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:38:03.606198Z",
     "iopub.status.busy": "2025-11-21T11:38:03.605620Z",
     "iopub.status.idle": "2025-11-21T11:38:03.632094Z",
     "shell.execute_reply": "2025-11-21T11:38:03.630885Z",
     "shell.execute_reply.started": "2025-11-21T11:38:03.606142Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comics to process: 3,929\n",
      "\n",
      "Comic order (first 30): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']\n",
      "Comic order (last 30):  ['3929', '3930', '3931', '3932', '3933', '3934', '3935', '3936', '3937', '3938', '3939', '3940', '3941', '3942', '3943', '3944', '3945', '3946', '3947', '3948', '3949', '3950', '3951', '3952', '3953', '3954', '3955', '3956', '3957', '3958']\n",
      "\n",
      "‚úÖ Comics sorted correctly (0, 1, 2, 3... not 0, 1, 10, 100...)\n"
     ]
    }
   ],
   "source": [
    "# Sort comics numerically\n",
    "sorted_comic_ids = sorted(comics_data.keys(), key=natural_sort_key)\n",
    "\n",
    "print(f\"Total comics to process: {len(sorted_comic_ids):,}\")\n",
    "print(f\"\\nComic order (first 30): {sorted_comic_ids[:30]}\")\n",
    "print(f\"Comic order (last 30):  {sorted_comic_ids[-30:]}\")\n",
    "print(f\"\\n‚úÖ Comics sorted correctly (0, 1, 2, 3... not 0, 1, 10, 100...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create-shards",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:38:28.194497Z",
     "iopub.status.busy": "2025-11-21T11:38:28.194081Z",
     "iopub.status.idle": "2025-11-21T11:38:51.393067Z",
     "shell.execute_reply": "2025-11-21T11:38:51.391873Z",
     "shell.execute_reply.started": "2025-11-21T11:38:28.194462Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî® Creating 35K shards...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:   4%|‚ñç         | 171/3929 [00:00<00:16, 233.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0000: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:   7%|‚ñã         | 284/3929 [00:01<00:17, 206.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0001: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  11%|‚ñà         | 428/3929 [00:02<00:17, 205.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0002: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  14%|‚ñà‚ñé        | 535/3929 [00:02<00:21, 159.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0003: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  17%|‚ñà‚ñã        | 652/3929 [00:03<00:19, 165.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0004: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  20%|‚ñà‚ñâ        | 767/3929 [00:04<00:18, 171.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0005: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  23%|‚ñà‚ñà‚ñé       | 885/3929 [00:04<00:17, 171.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0006: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  26%|‚ñà‚ñà‚ñå       | 1014/3929 [00:05<00:15, 192.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0007: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  29%|‚ñà‚ñà‚ñâ       | 1142/3929 [00:06<00:16, 173.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0008: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  31%|‚ñà‚ñà‚ñà‚ñè      | 1231/3929 [00:06<00:17, 158.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0009: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  35%|‚ñà‚ñà‚ñà‚ñç      | 1369/3929 [00:07<00:13, 193.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0010: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  37%|‚ñà‚ñà‚ñà‚ñã      | 1458/3929 [00:08<00:17, 139.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0011: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  40%|‚ñà‚ñà‚ñà‚ñà      | 1586/3929 [00:08<00:13, 169.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0012: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1691/3929 [00:09<00:14, 159.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0013: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1824/3929 [00:10<00:10, 208.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0014: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1945/3929 [00:10<00:11, 178.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0015: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2067/3929 [00:11<00:10, 182.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0016: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2157/3929 [00:12<00:11, 150.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0017: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2275/3929 [00:12<00:10, 154.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0018: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2378/3929 [00:13<00:09, 163.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0019: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2466/3929 [00:14<00:11, 129.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0020: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2585/3929 [00:14<00:08, 166.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0021: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2674/3929 [00:15<00:08, 142.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0022: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2807/3929 [00:16<00:05, 193.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0023: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2932/3929 [00:17<00:05, 182.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0024: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3041/3929 [00:17<00:05, 164.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0025: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3172/3929 [00:18<00:04, 176.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0026: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3302/3929 [00:19<00:03, 161.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0027: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3404/3929 [00:19<00:03, 153.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0028: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3513/3929 [00:20<00:02, 147.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0029: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3629/3929 [00:21<00:02, 146.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0030: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3738/3929 [00:22<00:01, 150.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0031: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3870/3929 [00:22<00:00, 180.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0032: 35,000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3929/3929 [00:23<00:00, 169.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Shard 0033: 24,843 images (final)\n",
      "\n",
      "================================================================================\n",
      "SHARDING COMPLETE\n",
      "================================================================================\n",
      "Total shards created:     34\n",
      "Images per shard:         35,000\n",
      "Total images written:     1,179,843\n",
      "\n",
      "Expected time per shard:  19-20 hours\n",
      "Safety buffer:            4-5 hours\n",
      "Timeout risk:             0% ‚úÖ\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüî® Creating 35K shards...\\n\")\n",
    "\n",
    "# Initialize shard writer\n",
    "shard_idx = 0\n",
    "current_shard_path = SHARDS_DIR / f\"shard_{shard_idx:04d}.jsonl\"\n",
    "shard_file = current_shard_path.open(\"w\", encoding=\"utf-8\")\n",
    "lines_in_shard = 0\n",
    "total_written = 0\n",
    "\n",
    "# Process each comic in sorted order\n",
    "for comic_id in tqdm(sorted_comic_ids, desc=\"Creating shards\"):\n",
    "    # Sort panels within comic by (page, panel)\n",
    "    panels = sorted(comics_data[comic_id], key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "    for page_num, panel_num, blob_name in panels:\n",
    "        req_line = make_request_line(blob_name)\n",
    "        shard_file.write(json.dumps(req_line) + \"\\n\")\n",
    "        lines_in_shard += 1\n",
    "        total_written += 1\n",
    "        \n",
    "        # Rotate shard at 35K\n",
    "        if lines_in_shard >= SHARD_SIZE:\n",
    "            shard_file.close()\n",
    "            print(f\"  ‚úÖ Shard {shard_idx:04d}: {lines_in_shard:,} images\")\n",
    "            \n",
    "            shard_idx += 1\n",
    "            current_shard_path = SHARDS_DIR / f\"shard_{shard_idx:04d}.jsonl\"\n",
    "            shard_file = current_shard_path.open(\"w\", encoding=\"utf-8\")\n",
    "            lines_in_shard = 0\n",
    "\n",
    "# Close final shard\n",
    "if lines_in_shard > 0:\n",
    "    shard_file.close()\n",
    "    print(f\"  ‚úÖ Shard {shard_idx:04d}: {lines_in_shard:,} images (final)\")\n",
    "    total_shards = shard_idx + 1\n",
    "else:\n",
    "    shard_file.close()\n",
    "    current_shard_path.unlink()\n",
    "    total_shards = shard_idx\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SHARDING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total shards created:     {total_shards}\")\n",
    "print(f\"Images per shard:         {SHARD_SIZE:,}\")\n",
    "print(f\"Total images written:     {total_written:,}\")\n",
    "print(f\"\\nExpected time per shard:  19-20 hours\")\n",
    "print(f\"Safety buffer:            4-5 hours\")\n",
    "print(f\"Timeout risk:             0% ‚úÖ\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è STEP 7: Upload Shards to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "upload-shards",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:39:45.950668Z",
     "iopub.status.busy": "2025-11-21T11:39:45.949605Z",
     "iopub.status.idle": "2025-11-21T11:39:58.808684Z",
     "shell.execute_reply": "2025-11-21T11:39:58.807470Z",
     "shell.execute_reply.started": "2025-11-21T11:39:45.950626Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è  Uploading shards to Cloud Storage...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:12<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Uploaded 34 shards to:\n",
      "   gs://harshasekar-comics-data/batch_inputs/optimized_35k/\n",
      "\n",
      "Shards are ready for batch submission!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚òÅÔ∏è  Uploading shards to Cloud Storage...\\n\")\n",
    "\n",
    "shard_files = sorted(SHARDS_DIR.glob(\"shard_*.jsonl\"))\n",
    "uploaded_paths = []\n",
    "\n",
    "for shard_file in tqdm(shard_files, desc=\"Uploading\"):\n",
    "    gcs_path = f\"{BATCH_INPUT_PREFIX}/{shard_file.name}\"\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.upload_from_filename(str(shard_file))\n",
    "    \n",
    "    full_uri = f\"gs://{BUCKET}/{gcs_path}\"\n",
    "    uploaded_paths.append(full_uri)\n",
    "\n",
    "print(f\"\\n‚úÖ Uploaded {len(uploaded_paths)} shards to:\")\n",
    "print(f\"   gs://{BUCKET}/{BATCH_INPUT_PREFIX}/\")\n",
    "print(f\"\\nShards are ready for batch submission!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "# üöÄ STEP 8: Generate Batch Submission Commands\n",
    "\n",
    "## This generates commands for 3 waves of submissions.\n",
    "## Copy-paste these commands into Cloud Shell to submit batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "generate-commands",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:40:11.950034Z",
     "iopub.status.busy": "2025-11-21T11:40:11.949621Z",
     "iopub.status.idle": "2025-11-21T11:40:11.963193Z",
     "shell.execute_reply": "2025-11-21T11:40:11.961846Z",
     "shell.execute_reply.started": "2025-11-21T11:40:11.949997Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ BATCH SUBMISSION COMMANDS\n",
      "================================================================================\n",
      "\n",
      "Total batches: 34\n",
      "Organized into 3 waves of 12 batches each\n",
      "\n",
      "\n",
      "================================================================================\n",
      "WAVE 1/3 - 12 BATCHES\n",
      "================================================================================\n",
      "\n",
      "# Batch 0000\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0000.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0000/\n",
      "\n",
      "# Batch 0001\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0001.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0001/\n",
      "\n",
      "# Batch 0002\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0002.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0002/\n",
      "\n",
      "# Batch 0003\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0003.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0003/\n",
      "\n",
      "# Batch 0004\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0004.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0004/\n",
      "\n",
      "# Batch 0005\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0005.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0005/\n",
      "\n",
      "# Batch 0006\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0006.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0006/\n",
      "\n",
      "# Batch 0007\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0007.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0007/\n",
      "\n",
      "# Batch 0008\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0008.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0008/\n",
      "\n",
      "# Batch 0009\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0009.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0009/\n",
      "\n",
      "# Batch 0010\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0010.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0010/\n",
      "\n",
      "# Batch 0011\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0011.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0011/\n",
      "\n",
      "\n",
      "‚è∞ After submitting Wave 1, wait ~20 hours before submitting Wave 2\n",
      "\n",
      "\n",
      "================================================================================\n",
      "WAVE 2/3 - 12 BATCHES\n",
      "================================================================================\n",
      "\n",
      "# Batch 0012\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0012.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0012/\n",
      "\n",
      "# Batch 0013\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0013.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0013/\n",
      "\n",
      "# Batch 0014\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0014.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0014/\n",
      "\n",
      "# Batch 0015\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0015.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0015/\n",
      "\n",
      "# Batch 0016\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0016.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0016/\n",
      "\n",
      "# Batch 0017\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0017.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0017/\n",
      "\n",
      "# Batch 0018\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0018.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0018/\n",
      "\n",
      "# Batch 0019\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0019.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0019/\n",
      "\n",
      "# Batch 0020\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0020.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0020/\n",
      "\n",
      "# Batch 0021\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0021.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0021/\n",
      "\n",
      "# Batch 0022\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0022.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0022/\n",
      "\n",
      "# Batch 0023\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0023.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0023/\n",
      "\n",
      "\n",
      "‚è∞ After submitting Wave 2, wait ~20 hours before submitting Wave 3\n",
      "\n",
      "\n",
      "================================================================================\n",
      "WAVE 3/3 - 10 BATCHES\n",
      "================================================================================\n",
      "\n",
      "# Batch 0024\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0024.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0024/\n",
      "\n",
      "# Batch 0025\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0025.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0025/\n",
      "\n",
      "# Batch 0026\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0026.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0026/\n",
      "\n",
      "# Batch 0027\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0027.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0027/\n",
      "\n",
      "# Batch 0028\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0028.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0028/\n",
      "\n",
      "# Batch 0029\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0029.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0029/\n",
      "\n",
      "# Batch 0030\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0030.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0030/\n",
      "\n",
      "# Batch 0031\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0031.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0031/\n",
      "\n",
      "# Batch 0032\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0032.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0032/\n",
      "\n",
      "# Batch 0033\n",
      "gcloud ai models batch-predict \\\n",
      "  --model=gemini-2.5-flash-lite \\\n",
      "  --project=fluent-justice-478703-f8 \\\n",
      "  --location=us-central1 \\\n",
      "  --input-uri=gs://harshasekar-comics-data/batch_inputs/optimized_35k/shard_0033.jsonl \\\n",
      "  --output-uri=gs://harshasekar-comics-data/ocr_outputs/optimized_35k/job_0033/\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUBMISSION TIMELINE\n",
      "================================================================================\n",
      "Wave 1: Submit 12 batches ‚Üí Wait ~20 hours\n",
      "Wave 2: Submit 12 batches ‚Üí Wait ~20 hours\n",
      "Wave 3: Submit 10 batches ‚Üí Wait ~20 hours\n",
      "\n",
      "Total time: ~60 hours (2.5 days)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ BATCH SUBMISSION COMMANDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group into waves of 12\n",
    "BATCHES_PER_WAVE = 12\n",
    "waves = [uploaded_paths[i:i+BATCHES_PER_WAVE] for i in range(0, len(uploaded_paths), BATCHES_PER_WAVE)]\n",
    "\n",
    "print(f\"\\nTotal batches: {len(uploaded_paths)}\")\n",
    "print(f\"Organized into {len(waves)} waves of {BATCHES_PER_WAVE} batches each\\n\")\n",
    "\n",
    "all_commands = []\n",
    "\n",
    "for wave_idx, wave_shards in enumerate(waves):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"WAVE {wave_idx + 1}/{len(waves)} - {len(wave_shards)} BATCHES\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for idx, shard_path in enumerate(wave_shards):\n",
    "        global_idx = wave_idx * BATCHES_PER_WAVE + idx\n",
    "        output_uri = f\"gs://{BUCKET}/{BATCH_OUTPUT_PREFIX}/job_{global_idx:04d}/\"\n",
    "        \n",
    "        cmd = f\"\"\"gcloud ai models batch-predict \\\\\n",
    "  --model=gemini-2.5-flash-lite \\\\\n",
    "  --project={PROJECT_ID} \\\\\n",
    "  --location={LOCATION} \\\\\n",
    "  --input-uri={shard_path} \\\\\n",
    "  --output-uri={output_uri}\"\"\"\n",
    "        \n",
    "        print(f\"# Batch {global_idx:04d}\")\n",
    "        print(cmd)\n",
    "        print()\n",
    "        \n",
    "        all_commands.append(cmd)\n",
    "    \n",
    "    if wave_idx < len(waves) - 1:\n",
    "        print(f\"\\n‚è∞ After submitting Wave {wave_idx + 1}, wait ~20 hours before submitting Wave {wave_idx + 2}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUBMISSION TIMELINE\")\n",
    "print(\"=\"*80)\n",
    "for i, wave in enumerate(waves):\n",
    "    print(f\"Wave {i+1}: Submit {len(wave)} batches ‚Üí Wait ~20 hours\")\n",
    "print(f\"\\nTotal time: ~{len(waves) * 20} hours ({len(waves) * 20 / 24:.1f} days)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "save-script",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:40:33.440391Z",
     "iopub.status.busy": "2025-11-21T11:40:33.439060Z",
     "iopub.status.idle": "2025-11-21T11:40:33.450913Z",
     "shell.execute_reply": "2025-11-21T11:40:33.449664Z",
     "shell.execute_reply.started": "2025-11-21T11:40:33.440302Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Commands saved to: submit_all_batches.sh\n",
      "\n",
      "You can also run: bash submit_all_batches.sh\n"
     ]
    }
   ],
   "source": [
    "# Save commands to script file\n",
    "script_file = WORKDIR / \"submit_all_batches.sh\"\n",
    "\n",
    "with script_file.open(\"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\\n\")\n",
    "    f.write(f\"# Generated: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"# Total batches: {len(all_commands)}\\n\")\n",
    "    f.write(f\"# Submit in waves of {BATCHES_PER_WAVE}\\n\\n\")\n",
    "    \n",
    "    for wave_idx, wave_shards in enumerate(waves):\n",
    "        f.write(f\"\\n# ===== WAVE {wave_idx + 1}/{len(waves)} =====\\n\\n\")\n",
    "        for idx, shard_path in enumerate(wave_shards):\n",
    "            global_idx = wave_idx * BATCHES_PER_WAVE + idx\n",
    "            output_uri = f\"gs://{BUCKET}/{BATCH_OUTPUT_PREFIX}/job_{global_idx:04d}/\"\n",
    "            f.write(f\"# Batch {global_idx:04d}\\n\")\n",
    "            f.write(f\"gcloud ai models batch-predict \\\\\\n\")\n",
    "            f.write(f\"  --model=gemini-2.5-flash-lite \\\\\\n\")\n",
    "            f.write(f\"  --project={PROJECT_ID} \\\\\\n\")\n",
    "            f.write(f\"  --location={LOCATION} \\\\\\n\")\n",
    "            f.write(f\"  --input-uri={shard_path} \\\\\\n\")\n",
    "            f.write(f\"  --output-uri={output_uri}\\n\\n\")\n",
    "        if wave_idx < len(waves) - 1:\n",
    "            f.write(f\"\\necho 'Wave {wave_idx + 1} submitted. Wait ~20 hours before Wave {wave_idx + 2}'\\n\")\n",
    "\n",
    "print(f\"\\nüíæ Commands saved to: {script_file}\")\n",
    "print(f\"\\nYou can also run: bash {script_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "961c4d8f-7b9f-4ff8-bb61-6dc4e2aef69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:50:35.308007Z",
     "iopub.status.busy": "2025-11-21T11:50:35.306954Z",
     "iopub.status.idle": "2025-11-21T11:50:38.579597Z",
     "shell.execute_reply": "2025-11-21T11:50:38.578160Z",
     "shell.execute_reply.started": "2025-11-21T11:50:35.307962Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUBMITTING WAVE 1 (Batches 0-11)\n",
      "================================================================================\n",
      "\n",
      "Submitting batch 0000...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/1225257326626209792 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0001...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/1382883313584177152 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0002...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/8953012074728914944 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0003...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/5737864153251446784 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0004...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/6629154667005739008 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0005...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/8020766951863222272 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0006...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/7172260634568949760 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0007...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/697913957758795776 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0008...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/4048592080522444800 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0009...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/2519620007030161408 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0010...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/4278275661518340096 (Status: JOB_STATE_PENDING)\n",
      "Submitting batch 0011...\n",
      "  ‚úÖ Job: projects/821865862314/locations/us-central1/batchPredictionJobs/2184524047256125440 (Status: JOB_STATE_PENDING)\n",
      "\n",
      "‚úÖ Submitted 12/12 batches\n",
      "Monitor: https://console.cloud.google.com/vertex-ai/batch-predictions\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SUBMIT WAVE 1 - Using Google GenAI SDK\n",
    "# ============================================================================\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import CreateBatchJobConfig\n",
    "\n",
    "# Initialize client\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUBMITTING WAVE 1 (Batches 0-11)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "jobs = []\n",
    "\n",
    "# Submit first 12 batches\n",
    "for i in range(12):\n",
    "    shard_uri = uploaded_paths[i]\n",
    "    output_prefix = f\"{BATCH_OUTPUT_PREFIX}/job_{i:04d}\"\n",
    "    \n",
    "    print(f\"Submitting batch {i:04d}...\")\n",
    "    \n",
    "    try:\n",
    "        job = client.batches.create(\n",
    "            model=\"gemini-2.5-flash-lite\",\n",
    "            src=shard_uri,\n",
    "            config=CreateBatchJobConfig(\n",
    "                dest=f\"gs://{BUCKET}/{output_prefix}\"\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ Job: {job.name} (Status: {job.state.name})\")\n",
    "        \n",
    "        jobs.append({\n",
    "            \"index\": i,\n",
    "            \"job_name\": job.name,\n",
    "            \"state\": job.state.name,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        break\n",
    "\n",
    "print()\n",
    "print(f\"‚úÖ Submitted {len(jobs)}/12 batches\")\n",
    "print(f\"Monitor: https://console.cloud.google.com/vertex-ai/batch-predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "# ‚è∏Ô∏è STOP HERE!\n",
    "\n",
    "## Next Steps:\n",
    "1. ‚úÖ Copy-paste Wave 1 commands (12 batches) into Cloud Shell\n",
    "2. ‚è∞ Wait ~20 hours for Wave 1 to complete\n",
    "3. ‚úÖ Copy-paste Wave 2 commands (12 batches)\n",
    "4. ‚è∞ Wait ~20 hours for Wave 2 to complete\n",
    "5. ‚úÖ Copy-paste Wave 3 commands (remaining batches)\n",
    "6. ‚è∞ Wait ~20 hours for Wave 3 to complete\n",
    "7. ‚úÖ Return to this notebook and run cells below to merge results\n",
    "\n",
    "## Monitor Progress:\n",
    "https://console.cloud.google.com/vertex-ai/batch-predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "# üì• STEP 9: Merge Results (Run AFTER all batches complete)\n",
    "\n",
    "## ‚ö†Ô∏è Only run these cells after ALL 35 batches are complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_prediction_files():\n",
    "    \"\"\"Finds all predictions.jsonl files in output directory.\"\"\"\n",
    "    prediction_files = []\n",
    "    blobs = bucket.list_blobs(prefix=BATCH_OUTPUT_PREFIX)\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if \"prediction\" in blob.name.lower() and blob.name.endswith(\".jsonl\"):\n",
    "            prediction_files.append(blob.name)\n",
    "    \n",
    "    return prediction_files\n",
    "\n",
    "print(\"üîç Finding all prediction output files...\")\n",
    "all_prediction_files = find_all_prediction_files()\n",
    "print(f\"\\n‚úÖ Found {len(all_prediction_files)} prediction files\")\n",
    "\n",
    "if len(all_prediction_files) < total_shards:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Expected {total_shards} files, found {len(all_prediction_files)}\")\n",
    "    print(\"   Some batches may still be running or failed\")\n",
    "    print(\"   Check console: https://console.cloud.google.com/vertex-ai/batch-predictions\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All expected prediction files found!\")\n",
    "    \n",
    "for pf in sorted(all_prediction_files)[:5]:\n",
    "    print(f\"   - {pf}\")\n",
    "if len(all_prediction_files) > 5:\n",
    "    print(f\"   ... and {len(all_prediction_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse-ocr",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gemini_ocr_line(jsonl_line: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parses Gemini batch output line into structured OCR data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rec = json.loads(jsonl_line)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    \n",
    "    custom_id = rec.get(\"custom_id\", \"\")\n",
    "    parts = custom_id.split(\"-\")\n",
    "    \n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    \n",
    "    comic_no = parts[0]\n",
    "    page_no = parts[1]\n",
    "    panel_no = parts[2]\n",
    "    img_path = f\"raw_panel_images/{comic_no}/{page_no}_{panel_no}.jpg\"\n",
    "    \n",
    "    response = rec.get(\"response\", {})\n",
    "    candidates = response.get(\"candidates\", [])\n",
    "    \n",
    "    if not candidates:\n",
    "        return {\n",
    "            \"comic_no\": comic_no, \"page_no\": page_no, \"panel_no\": panel_no,\n",
    "            \"img_path\": img_path, \"agg_text\": \"\", \"bubble_count\": 0,\n",
    "            \"bubbles_json\": json.dumps({\"bubbles\": []}),\n",
    "        }\n",
    "    \n",
    "    content = candidates[0].get(\"content\", {})\n",
    "    parts_list = content.get(\"parts\", [])\n",
    "    \n",
    "    if not parts_list:\n",
    "        return {\n",
    "            \"comic_no\": comic_no, \"page_no\": page_no, \"panel_no\": panel_no,\n",
    "            \"img_path\": img_path, \"agg_text\": \"\", \"bubble_count\": 0,\n",
    "            \"bubbles_json\": json.dumps({\"bubbles\": []}),\n",
    "        }\n",
    "    \n",
    "    raw_text = parts_list[0].get(\"text\", \"\")\n",
    "    bubbles_data = {\"bubbles\": []}\n",
    "    \n",
    "    try:\n",
    "        match = re.search(r\"```(?:json)?\\s*({.*?})\\s*```\", raw_text, re.DOTALL)\n",
    "        if match:\n",
    "            bubbles_data = json.loads(match.group(1))\n",
    "        else:\n",
    "            bubbles_data = json.loads(raw_text)\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        pass\n",
    "    \n",
    "    bubbles = bubbles_data.get(\"bubbles\", [])\n",
    "    agg_text = \" \".join(b.get(\"text\", \"\") for b in bubbles)\n",
    "    \n",
    "    return {\n",
    "        \"comic_no\": comic_no, \"page_no\": page_no, \"panel_no\": panel_no,\n",
    "        \"img_path\": img_path, \"agg_text\": agg_text, \"bubble_count\": len(bubbles),\n",
    "        \"bubbles_json\": json.dumps(bubbles_data),\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ OCR parser configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_RAW_CSV = Path(\"COMICS_OCR_MASTER_raw.csv\")\n",
    "SHARD_STATS_CSV = Path(\"COMICS_OCR_SHARD_STATS.csv\")\n",
    "\n",
    "print(\"üîÑ Merging all prediction files into master CSV...\\n\")\n",
    "\n",
    "master_f = MASTER_RAW_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\")\n",
    "writer = csv.writer(master_f)\n",
    "writer.writerow([\n",
    "    \"comic_no\", \"page_no\", \"panel_no\",\n",
    "    \"img_path\", \"agg_text\", \"bubble_count\", \"bubbles_json\"\n",
    "])\n",
    "\n",
    "shard_stats = []\n",
    "\n",
    "for blob_name in tqdm(all_prediction_files, desc=\"Merging predictions\"):\n",
    "    blob = bucket.blob(blob_name)\n",
    "    text = blob.download_as_text(encoding=\"utf-8\")\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    n_panels = 0\n",
    "    total_prompt_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        parsed = parse_gemini_ocr_line(line)\n",
    "        if not parsed:\n",
    "            continue\n",
    "        \n",
    "        writer.writerow([\n",
    "            parsed[\"comic_no\"], parsed[\"page_no\"], parsed[\"panel_no\"],\n",
    "            parsed[\"img_path\"], parsed[\"agg_text\"], parsed[\"bubble_count\"],\n",
    "            parsed[\"bubbles_json\"],\n",
    "        ])\n",
    "\n",
    "        um = rec.get(\"response\", {}).get(\"usageMetadata\", {})\n",
    "        total_prompt_tokens += um.get(\"promptTokenCount\", 0)\n",
    "        total_output_tokens += um.get(\"candidatesTokenCount\", 0)\n",
    "        total_tokens += um.get(\"totalTokenCount\", 0)\n",
    "        n_panels += 1\n",
    "\n",
    "    shard_stats.append({\n",
    "        \"blob_name\": blob_name, \"num_panels\": n_panels,\n",
    "        \"total_prompt_tokens\": total_prompt_tokens,\n",
    "        \"total_output_tokens\": total_output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"avg_total_tokens_per_panel\": (total_tokens / n_panels) if n_panels else 0.0,\n",
    "    })\n",
    "\n",
    "master_f.close()\n",
    "\n",
    "# Write stats\n",
    "with SHARD_STATS_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as sf:\n",
    "    sw = csv.writer(sf)\n",
    "    sw.writerow([\n",
    "        \"blob_name\", \"num_panels\", \"total_prompt_tokens\", \"total_output_tokens\",\n",
    "        \"total_tokens\", \"avg_total_tokens_per_panel\"\n",
    "    ])\n",
    "    for st in shard_stats:\n",
    "        sw.writerow([\n",
    "            st[\"blob_name\"], st[\"num_panels\"], st[\"total_prompt_tokens\"],\n",
    "            st[\"total_output_tokens\"], st[\"total_tokens\"], st[\"avg_total_tokens_per_panel\"],\n",
    "        ])\n",
    "\n",
    "print(f\"\\n‚úÖ Master CSV: {MASTER_RAW_CSV}\")\n",
    "print(f\"‚úÖ Stats CSV: {SHARD_STATS_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate costs\n",
    "stats_df = pd.read_csv(SHARD_STATS_CSV)\n",
    "\n",
    "TOTAL_PANELS = stats_df[\"num_panels\"].sum()\n",
    "TOTAL_TOKENS = stats_df[\"total_tokens\"].sum()\n",
    "TOTAL_INPUT_TOKENS = stats_df[\"total_prompt_tokens\"].sum()\n",
    "TOTAL_OUTPUT_TOKENS = stats_df[\"total_output_tokens\"].sum()\n",
    "\n",
    "# Gemini 2.5 Flash-Lite with 50% batch discount\n",
    "INPUT_PRICE = 0.00625   # per 1M tokens\n",
    "OUTPUT_PRICE = 0.025    # per 1M tokens\n",
    "\n",
    "INPUT_COST = TOTAL_INPUT_TOKENS / 1_000_000 * INPUT_PRICE\n",
    "OUTPUT_COST = TOTAL_OUTPUT_TOKENS / 1_000_000 * OUTPUT_PRICE\n",
    "TOTAL_COST = INPUT_COST + OUTPUT_COST\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ FINAL PROJECT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Total panels processed:    {TOTAL_PANELS:>12,}\")\n",
    "print(f\"Total tokens used:         {TOTAL_TOKENS:>12,}\")\n",
    "print(f\"Avg tokens per panel:      {TOTAL_TOKENS/TOTAL_PANELS:>12,.1f}\")\n",
    "print()\n",
    "print(f\"Input tokens:              {TOTAL_INPUT_TOKENS:>12,}  ‚Üí  ${INPUT_COST:>8,.2f}\")\n",
    "print(f\"Output tokens:             {TOTAL_OUTPUT_TOKENS:>12,}  ‚Üí  ${OUTPUT_COST:>8,.2f}\")\n",
    "print(f\"\\nTOTAL PROJECT COST:        {' '*12}      ${TOTAL_COST:>8,.2f}\")\n",
    "print(f\"Original budget:           {' '*12}      ${164.11:>8,.2f}\")\n",
    "print(f\"\\nStatus: {'‚úÖ ON BUDGET' if TOTAL_COST <= 164.11 else '‚ö†Ô∏è  SLIGHTLY OVER'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sort-csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort master CSV by comic/page/panel\n",
    "MASTER_SORTED_CSV = Path(\"COMICS_OCR_MASTER_sorted.csv\")\n",
    "\n",
    "print(\"üìä Sorting master CSV by comic_no, page_no, panel_no...\")\n",
    "\n",
    "df = pd.read_csv(MASTER_RAW_CSV)\n",
    "\n",
    "# Convert to numeric for proper sorting\n",
    "df[\"comic_no\"] = pd.to_numeric(df[\"comic_no\"], errors=\"coerce\")\n",
    "df[\"page_no\"] = pd.to_numeric(df[\"page_no\"], errors=\"coerce\")\n",
    "df[\"panel_no\"] = pd.to_numeric(df[\"panel_no\"], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"comic_no\", \"page_no\", \"panel_no\"], ascending=[True, True, True])\n",
    "df.to_csv(MASTER_SORTED_CSV, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Final sorted CSV: {MASTER_SORTED_CSV}\")\n",
    "print(f\"   Total rows: {len(df):,}\")\n",
    "print(\"\\nüéØ Dataset ready for LLaVA/OpenFlamingo training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"\\nüìä Data Preview:\\n\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\nüìà Dataset Statistics:\\n\")\n",
    "print(f\"Total panels: {len(df):,}\")\n",
    "print(f\"Unique comics: {df['comic_no'].nunique():,}\")\n",
    "print(f\"Panels with text: {(df['bubble_count'] > 0).sum():,}\")\n",
    "print(f\"Panels without text: {(df['bubble_count'] == 0).sum():,}\")\n",
    "print(f\"Average bubbles per panel: {df['bubble_count'].mean():.2f}\")\n",
    "print(f\"\\nText length distribution:\")\n",
    "df['text_length'] = df['agg_text'].str.len()\n",
    "print(df[df['text_length'] > 0]['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completion",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ PIPELINE COMPLETE!\n",
    "\n",
    "## What You Have:\n",
    "- ‚úÖ Complete ~1.2M image dataset with OCR\n",
    "- ‚úÖ Master CSV sorted by comic/page/panel\n",
    "- ‚úÖ Full cost and token analysis\n",
    "- ‚úÖ 0% timeout rate (35K batch size worked!)\n",
    "- ‚úÖ Comics processed in correct numerical order\n",
    "- ‚úÖ macOS metadata files properly filtered\n",
    "\n",
    "## Files Created:\n",
    "1. **COMICS_OCR_MASTER_sorted.csv** - Main dataset (download this!)\n",
    "2. **COMICS_OCR_SHARD_STATS.csv** - Token usage statistics\n",
    "3. **submit_all_batches.sh** - Batch submission commands\n",
    "\n",
    "## Next Steps:\n",
    "1. Download `COMICS_OCR_MASTER_sorted.csv`\n",
    "2. Sample 100 comics (~35K panels) for fine-tuning\n",
    "3. Start LLaVA/OpenFlamingo training\n",
    "4. Complete research by Dec 2 deadline! üéØ\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
