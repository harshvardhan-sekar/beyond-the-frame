===========================================
COMICS FINE-TUNING WITH LoRA (1 x H200 GPU)
===========================================
Job ID: 14122749
Node: gpue01
Started: Tue Dec 16 16:37:23 CST 2025
===========================================

GPU Info:
name, memory.total [MiB]
NVIDIA H200, 143771 MiB

======================================================================
COMICS FINE-TUNING WITH LoRA (v2 - WITH LABEL MASKING FIX)
======================================================================

PyTorch: 2.5.1+cu121
CUDA available: True
Number of GPUs: 1
World size: 1
  GPU 0: NVIDIA H200

Loading processor...
Loading model...
Model loaded on GPU 0

Preparing model for LoRA...
trainable params: 43,245,568 || all params: 8,074,053,152 || trainable%: 0.5356

Loading training data...
Loaded 249,576 training sequences
Dataset: 10000 valid sequences

======================================================================
VERIFYING LABEL MASKING (first example)
======================================================================
  Total tokens: 13576
  Masked (prompt): 13430 (98.9%)
  Active (response): 146 (1.1%)
  ✅ Label masking is working correctly!
======================================================================


======================================================================
STARTING DISTRIBUTED TRAINING
======================================================================
Train examples: 10000
GPUs: 1
Batch per GPU: 4
Gradient accumulation: 4
Effective batch size: 16
Context panels: 5
Max image size: 672  <-- reduced from 672
Epochs: 1
Total steps: ~625
======================================================================

{'loss': 1.6399, 'grad_norm': 1.4658751487731934, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.02}
{'loss': 1.6424, 'grad_norm': 1.0894542932510376, 'learning_rate': 6.031746031746032e-06, 'epoch': 0.03}
{'loss': 1.6318, 'grad_norm': 1.516566276550293, 'learning_rate': 9.206349206349207e-06, 'epoch': 0.05}
{'loss': 1.5416, 'grad_norm': 1.7024370431900024, 'learning_rate': 1.2380952380952383e-05, 'epoch': 0.06}
{'loss': 1.4958, 'grad_norm': 1.5497297048568726, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.08}
{'loss': 1.4242, 'grad_norm': 1.4481736421585083, 'learning_rate': 1.8730158730158732e-05, 'epoch': 0.1}
{'loss': 1.3385, 'grad_norm': 1.396064281463623, 'learning_rate': 1.9994375823958504e-05, 'epoch': 0.11}
{'loss': 1.3865, 'grad_norm': 1.5073039531707764, 'learning_rate': 1.9960028766541336e-05, 'epoch': 0.13}
{'loss': 1.3221, 'grad_norm': 1.401145339012146, 'learning_rate': 1.9894566364711965e-05, 'epoch': 0.14}
{'loss': 1.3462, 'grad_norm': 1.7891737222671509, 'learning_rate': 1.9798193124423804e-05, 'epoch': 0.16}
{'loss': 1.2969, 'grad_norm': 2.0367655754089355, 'learning_rate': 1.967121011775546e-05, 'epoch': 0.18}
{'loss': 1.2711, 'grad_norm': 1.7239999771118164, 'learning_rate': 1.9514014042355057e-05, 'epoch': 0.19}
{'loss': 1.2286, 'grad_norm': 1.783280372619629, 'learning_rate': 1.9327095982148258e-05, 'epoch': 0.21}
{'loss': 1.3131, 'grad_norm': 1.871172308921814, 'learning_rate': 1.9111039873181478e-05, 'epoch': 0.22}
{'loss': 1.2985, 'grad_norm': 2.033716917037964, 'learning_rate': 1.8866520679393127e-05, 'epoch': 0.24}
{'loss': 1.2506, 'grad_norm': 1.8426039218902588, 'learning_rate': 1.8594302284011704e-05, 'epoch': 0.26}
{'loss': 1.2432, 'grad_norm': 2.185687780380249, 'learning_rate': 1.829523510316813e-05, 'epoch': 0.27}
{'loss': 1.2453, 'grad_norm': 1.5749238729476929, 'learning_rate': 1.7970253429177477e-05, 'epoch': 0.29}
{'loss': 1.2156, 'grad_norm': 2.0429928302764893, 'learning_rate': 1.7620372511789607e-05, 'epoch': 0.3}
{'loss': 1.237, 'grad_norm': 2.4551279544830322, 'learning_rate': 1.7246685386527098e-05, 'epoch': 0.32}
{'loss': 1.2263, 'grad_norm': 2.2155699729919434, 'learning_rate': 1.6850359460018737e-05, 'epoch': 0.34}
{'loss': 1.1942, 'grad_norm': 2.208160161972046, 'learning_rate': 1.6432632862996056e-05, 'epoch': 0.35}
{'loss': 1.1881, 'grad_norm': 1.7950639724731445, 'learning_rate': 1.599481058234626e-05, 'epoch': 0.37}
{'loss': 1.2202, 'grad_norm': 1.961051106452942, 'learning_rate': 1.5538260384305076e-05, 'epoch': 0.38}
{'loss': 1.2134, 'grad_norm': 2.1593210697174072, 'learning_rate': 1.5064408541525573e-05, 'epoch': 0.4}
{'loss': 1.2206, 'grad_norm': 2.454453229904175, 'learning_rate': 1.457473537737167e-05, 'epoch': 0.42}
{'loss': 1.2285, 'grad_norm': 1.9307488203048706, 'learning_rate': 1.407077064135607e-05, 'epoch': 0.43}
{'loss': 1.217, 'grad_norm': 1.960749626159668, 'learning_rate': 1.3554088730169814e-05, 'epoch': 0.45}
{'loss': 1.2176, 'grad_norm': 1.9307359457015991, 'learning_rate': 1.3026303769233112e-05, 'epoch': 0.46}
{'loss': 1.2795, 'grad_norm': 1.9342982769012451, 'learning_rate': 1.2489064570132764e-05, 'epoch': 0.48}
{'loss': 1.201, 'grad_norm': 3.1571357250213623, 'learning_rate': 1.1944049479699244e-05, 'epoch': 0.5}
{'loss': 1.2042, 'grad_norm': 2.0754201412200928, 'learning_rate': 1.1392961136815046e-05, 'epoch': 0.51}
{'loss': 1.2047, 'grad_norm': 2.0605509281158447, 'learning_rate': 1.0837521153334143e-05, 'epoch': 0.53}
{'loss': 1.2224, 'grad_norm': 2.1203930377960205, 'learning_rate': 1.0279464735729472e-05, 'epoch': 0.54}
{'loss': 1.2091, 'grad_norm': 2.2290258407592773, 'learning_rate': 9.720535264270529e-06, 'epoch': 0.56}
{'loss': 1.1909, 'grad_norm': 2.660459280014038, 'learning_rate': 9.16247884666586e-06, 'epoch': 0.58}
{'loss': 1.2022, 'grad_norm': 2.2446398735046387, 'learning_rate': 8.607038863184957e-06, 'epoch': 0.59}
{'loss': 1.1815, 'grad_norm': 2.2652761936187744, 'learning_rate': 8.05595052030076e-06, 'epoch': 0.61}
{'loss': 1.1953, 'grad_norm': 2.1247003078460693, 'learning_rate': 7.510935429867237e-06, 'epoch': 0.62}
{'loss': 1.186, 'grad_norm': 2.1858317852020264, 'learning_rate': 6.973696230766891e-06, 'epoch': 0.64}
{'loss': 1.1996, 'grad_norm': 2.166348934173584, 'learning_rate': 6.445911269830189e-06, 'epoch': 0.66}
{'loss': 1.1341, 'grad_norm': 2.103365898132324, 'learning_rate': 5.929229358643932e-06, 'epoch': 0.67}
{'loss': 1.1614, 'grad_norm': 2.1489832401275635, 'learning_rate': 5.42526462262833e-06, 'epoch': 0.69}
{'loss': 1.2222, 'grad_norm': 2.2629220485687256, 'learning_rate': 4.935591458474433e-06, 'epoch': 0.7}
{'loss': 1.1792, 'grad_norm': 2.4271464347839355, 'learning_rate': 4.461739615694929e-06, 'epoch': 0.72}
{'loss': 1.2064, 'grad_norm': 2.1740317344665527, 'learning_rate': 4.005189417653743e-06, 'epoch': 0.74}
{'loss': 1.1517, 'grad_norm': 1.9642877578735352, 'learning_rate': 3.5673671370039464e-06, 'epoch': 0.75}
{'loss': 1.1934, 'grad_norm': 2.4236257076263428, 'learning_rate': 3.149640539981267e-06, 'epoch': 0.77}
{'loss': 1.1768, 'grad_norm': 2.6822450160980225, 'learning_rate': 2.753314613472906e-06, 'epoch': 0.78}
{'loss': 1.1785, 'grad_norm': 2.442910671234131, 'learning_rate': 2.3796274882103964e-06, 'epoch': 0.8}
{'loss': 1.1714, 'grad_norm': 2.5647761821746826, 'learning_rate': 2.029746570822524e-06, 'epoch': 0.82}
{'loss': 1.194, 'grad_norm': 2.0808606147766113, 'learning_rate': 1.7047648968318697e-06, 'epoch': 0.83}
{'loss': 1.1539, 'grad_norm': 6.18720817565918, 'learning_rate': 1.4056977159883011e-06, 'epoch': 0.85}
{'loss': 1.1561, 'grad_norm': 1.884218692779541, 'learning_rate': 1.1334793206068739e-06, 'epoch': 0.86}
{'loss': 1.2191, 'grad_norm': 2.277831554412842, 'learning_rate': 8.889601268185233e-07, 'epoch': 0.88}
{'loss': 1.1951, 'grad_norm': 2.079030990600586, 'learning_rate': 6.729040178517454e-07, 'epoch': 0.9}
{'loss': 1.195, 'grad_norm': 2.856245994567871, 'learning_rate': 4.859859576449444e-07, 'epoch': 0.91}
{'loss': 1.1566, 'grad_norm': 2.2913551330566406, 'learning_rate': 3.2878988224454346e-07, 'epoch': 0.93}
{'loss': 1.2099, 'grad_norm': 2.5027987957000732, 'learning_rate': 2.0180687557619816e-07, 'epoch': 0.94}
{'loss': 1.165, 'grad_norm': 2.3615381717681885, 'learning_rate': 1.0543363528803696e-07, 'epoch': 0.96}
{'loss': 1.1962, 'grad_norm': 2.173938274383545, 'learning_rate': 3.9971233458665495e-08, 'epoch': 0.98}
{'loss': 1.2105, 'grad_norm': 2.3582348823547363, 'learning_rate': 5.6241760414987856e-09, 'epoch': 0.99}
{'train_runtime': 11533.1591, 'train_samples_per_second': 0.867, 'train_steps_per_second': 0.054, 'train_loss': 1.2510674285888672, 'epoch': 1.0}

Training completed in 192.2 minutes

Saving LoRA adapter...

✅ LoRA adapter saved to: /scratch/bftl/hsekar/comics_project/checkpoints/llava_comics_lora_pred

======================================================================
TRAINING COMPLETE!
======================================================================

===========================================
Finished: Tue Dec 16 19:53:19 CST 2025
===========================================
