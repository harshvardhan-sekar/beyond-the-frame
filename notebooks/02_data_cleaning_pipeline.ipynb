{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ðŸ“¦ Data Cleaning Pipeline for COMICS Fine-Tuning\n",
    "\n",
    "This notebook prepares clean, high-quality training data by:\n",
    "\n",
    "1. **Removing cover pages** (page 0)\n",
    "2. **Detecting story boundaries** (page gaps + title patterns)\n",
    "3. **Filtering advertisement panels** (ad keyword detection)\n",
    "4. **Skipping short segments** (<10 panels)\n",
    "5. **Building clean sequences** (only within story segments)\n",
    "6. **Including OCR text** in prompts for context\n",
    "\n",
    "**Note:** Story boundary detection uses PATTERNS, not just character names!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell1-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "  OCR CSV: /scratch/bftl/hsekar/comics_project/data/ocr/COMICS_OCR_WAVE1_sorted.csv\n",
      "  Images: /scratch/bftl/hsekar/comics_project/data/images\n",
      "  Model: llava-hf/llava-v1.6-mistral-7b-hf\n",
      "  Context window: 5\n",
      "  Min segment length: 10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 1: CONFIGURATION\n",
    "=============================================================================\n",
    "Set your parameters here before running the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = Path(\"/scratch/bftl/hsekar/comics_project/data\")\n",
    "OCR_CSV_PATH = DATA_DIR / \"ocr\" / \"COMICS_OCR_WAVE1_sorted.csv\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUT_DIR = Path(\"/scratch/bftl/hsekar/comics_project/outputs\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# SEQUENCE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONTEXT_WINDOW = 5          # Number of context panels before target\n",
    "MIN_SEGMENT_LENGTH = 10     # Minimum panels in a story segment to use it\n",
    "MIN_PAGE_NUMBER = 1         # Skip page 0 (cover) - start from page 1\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET SPLIT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Using Mistral based LLaVA (better for dialogue generation)\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "MODEL_CACHE = Path(\"/scratch/bftl/hsekar/comics_project/model_cache\")\n",
    "\n",
    "# =============================================================================\n",
    "# FOR QUICK TESTING (set to None to use all comics)\n",
    "# =============================================================================\n",
    "\n",
    "MAX_COMICS = None  # Set to e.g., 50 for testing, None for full dataset\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"  OCR CSV: {OCR_CSV_PATH}\")\n",
    "print(f\"  Images: {IMAGES_DIR}\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  Context window: {CONTEXT_WINDOW}\")\n",
    "print(f\"  Min segment length: {MIN_SEGMENT_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell2-filters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter functions loaded!\n",
      "  Title patterns: 30\n",
      "  Story end patterns: 5\n",
      "  Ad keywords: 67\n",
      "\n",
      "Test cases:\n",
      "  'BLUE BEETLE IN: THE MYSTERY CASE...' -> True\n",
      "  'CAPTAIN MARVEL VS THE VILLAIN...' -> True\n",
      "  'THE AMAZING SPIDER-MAN...' -> True\n",
      "  'CHAPTER 3: THE FINAL BATTLE...' -> True\n",
      "  'BY JACK KIRBY...' -> True\n",
      "  'HERCULES MODERN CHAMPION OF JUSTICE...' -> True\n",
      "  'SOME RANDOM DIALOGUE HERE...' -> True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 2: FILTER FUNCTIONS (ROBUST VERSION)\n",
    "=============================================================================\n",
    "Core filtering logic for cleaning the data.\n",
    "\n",
    "IMPORTANT: Story detection uses PATTERNS that work for ANY character,\n",
    "not just a limited list of known names!\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "# =============================================================================\n",
    "# GENERIC TITLE PATTERNS (work for ANY character/story)\n",
    "# =============================================================================\n",
    "\n",
    "# These patterns indicate a NEW STORY is starting - works regardless of character\n",
    "TITLE_INDICATOR_PATTERNS = [\n",
    "    # Story introduction patterns\n",
    "    r'\\bIN:\\s',              # \"BATMAN IN: THE DARK NIGHT\"\n",
    "    r'\\bSTARRING\\b',         # \"STARRING THE BLUE BEETLE\"\n",
    "    r'\\bPRESENTS\\b',         # \"MARVEL PRESENTS\"\n",
    "    r'\\bFEATURING\\b',        # \"FEATURING CAPTAIN AMERICA\"\n",
    "    r'\\bINTRODUCING\\b',      # \"INTRODUCING THE NEW HERO\"\n",
    "    \n",
    "    # Chapter/Part indicators\n",
    "    r'\\bCHAPTER\\s+[\\dIVX]+', # \"CHAPTER 1\", \"CHAPTER IV\"\n",
    "    r'\\bPART\\s+[\\dIVX]+',    # \"PART 1\", \"PART II\"\n",
    "    r'\\bEPISODE\\s+[\\dIVX]+', # \"EPISODE 5\"\n",
    "    \n",
    "    # Versus patterns\n",
    "    r'\\bVS\\.?\\b',            # \"HERO VS VILLAIN\", \"HERO VS. VILLAIN\"\n",
    "    r'\\bVERSUS\\b',           # \"HERO VERSUS VILLAIN\"\n",
    "    \n",
    "    # Adventure patterns\n",
    "    r'THE ADVENTURES? OF\\b', # \"THE ADVENTURE OF\", \"THE ADVENTURES OF\"\n",
    "    r'\\bMEETS?\\b',           # \"BATMAN MEETS SUPERMAN\"\n",
    "    \n",
    "    # Author/credit patterns (indicate story start)\n",
    "    r'\\bBY\\s+[A-Z][A-Z]+',   # \"BY KIRBY\", \"BY SIMON\"\n",
    "    r'WRITTEN BY\\b',\n",
    "    r'DRAWN BY\\b',\n",
    "    r'ART BY\\b',\n",
    "    r'STORY BY\\b',\n",
    "    \n",
    "    # Common title prefixes (work for any character)\n",
    "    r'^THE\\s+AMAZING\\b',     # \"THE AMAZING SPIDER-MAN\"\n",
    "    r'^THE\\s+INCREDIBLE\\b',  # \"THE INCREDIBLE HULK\"\n",
    "    r'^THE\\s+MIGHTY\\b',      # \"THE MIGHTY THOR\"\n",
    "    r'^THE\\s+SPECTACULAR\\b', # \"THE SPECTACULAR...\"\n",
    "    r'^THE\\s+FANTASTIC\\b',   # \"THE FANTASTIC FOUR\"\n",
    "    r'^THE\\s+UNCANNY\\b',     # \"THE UNCANNY X-MEN\"\n",
    "    \n",
    "    # Military/professional titles (work for many characters)\n",
    "    r'^CAPTAIN\\s+[A-Z]',     # \"CAPTAIN AMERICA\", \"CAPTAIN MARVEL\"\n",
    "    r'^SERGEANT\\s+[A-Z]',    # \"SERGEANT FURY\"\n",
    "    r'^CORPORAL\\s+[A-Z]',    # \"CORPORAL COLLINS\"\n",
    "    r'^DOCTOR\\s+[A-Z]',      # \"DOCTOR STRANGE\"\n",
    "    r'^DETECTIVE\\s+[A-Z]',   # \"DETECTIVE COMICS\"\n",
    "    r'^AGENT\\s+[A-Z]',       # \"AGENT X\"\n",
    "    r'^PROFESSOR\\s+[A-Z]',   # \"PROFESSOR X\"\n",
    "]\n",
    "\n",
    "# Patterns that indicate END of a story\n",
    "STORY_END_PATTERNS = [\n",
    "    r'\\bTHE\\s+END\\b',\n",
    "    r'^END$',\n",
    "    r'\\bFINIS\\b',\n",
    "    r'\\bTO\\s+BE\\s+CONTINUED\\b',\n",
    "    r'\\bCONTINUED\\s+NEXT\\b',\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# ADVERTISEMENT KEYWORDS\n",
    "# =============================================================================\n",
    "\n",
    "AD_KEYWORDS = [\n",
    "    # Sales/commerce\n",
    "    'BUY NOW', 'ORDER NOW', 'ORDER TODAY',\n",
    "    'SEND FOR', 'SEND ONLY', 'SEND JUST',\n",
    "    'ONLY $', 'JUST $', 'ONLY 10', 'ONLY 25', 'ONLY 50',\n",
    "    'FREE GIFT', 'FREE CATALOG', 'FREE BOOK', 'FREE SAMPLE',\n",
    "    'MONEY BACK', 'GUARANTEE', 'GUARANTEED',\n",
    "    'COUPON', 'CLIP THIS', 'CUT OUT',\n",
    "    'MAIL TO', 'MAIL THIS', 'SEND TO',\n",
    "    'P.O. BOX', 'POST OFFICE BOX', 'BOX NO.',\n",
    "    \n",
    "    # Product categories common in old comics\n",
    "    'BODY BUILDING', 'BUILD MUSCLE', 'CHARLES ATLAS',\n",
    "    'X-RAY SPECS', 'X-RAY VISION', 'SEE THRU',\n",
    "    'BB GUN', 'AIR RIFLE', 'DAISY GUN',\n",
    "    'STAMP COLLECT', 'COIN COLLECT',\n",
    "    'MAGIC TRICK', 'LEARN MAGIC',\n",
    "    'SEA MONKEY', 'SEA-MONKEY', 'SEAMONKEY',\n",
    "    \n",
    "    # War-era ads\n",
    "    'WAR BONDS', 'WAR STAMPS', 'BUY BONDS',\n",
    "    'DEFENSE BONDS', 'SAVINGS BONDS',\n",
    "    \n",
    "    # Address patterns (ads have mailing addresses)\n",
    "    'DEPT.', 'DEPARTMENT',\n",
    "    'NEW YORK, N.Y', 'CHICAGO, ILL', 'NEWARK, N.J',\n",
    "    \n",
    "    # Pricing patterns\n",
    "    '10Â¢', '25Â¢', '50Â¢', '98Â¢', '$1.00', '$1.98',\n",
    "    '10 CENTS', '25 CENTS', '50 CENTS',\n",
    "    \n",
    "    # Subscription/membership\n",
    "    'SUBSCRIBE', 'SUBSCRIPTION',\n",
    "    'JOIN NOW', 'JOIN THE CLUB',\n",
    "    'MEMBERSHIP',\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# FILTER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def is_likely_advertisement(text):\n",
    "    \"\"\"\n",
    "    Detect if a panel is likely an advertisement.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    text_upper = text.upper()\n",
    "    \n",
    "    for keyword in AD_KEYWORDS:\n",
    "        if keyword in text_upper:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def is_story_title_panel(text):\n",
    "    \"\"\"\n",
    "    Detect if a panel is likely a story title/splash page.\n",
    "    \n",
    "    Uses MULTIPLE SIGNALS that work for ANY character:\n",
    "    1. Regex patterns for common title structures\n",
    "    2. Short + mostly ALL CAPS text\n",
    "    3. Story ending patterns (next page is new story)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if panel appears to be a story title\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    text_upper = text.upper().strip()\n",
    "    \n",
    "    # Signal 1: Check regex patterns (works for ANY character)\n",
    "    for pattern in TITLE_INDICATOR_PATTERNS:\n",
    "        if re.search(pattern, text_upper):\n",
    "            return True\n",
    "    \n",
    "    # Signal 2: Short text that is mostly ALL CAPS (classic title format)\n",
    "    words = text.split()\n",
    "    if 2 <= len(words) <= 10:  # Title length range\n",
    "        caps_chars = sum(1 for c in text if c.isupper())\n",
    "        alpha_chars = sum(1 for c in text if c.isalpha())\n",
    "        if alpha_chars > 0:\n",
    "            caps_ratio = caps_chars / alpha_chars\n",
    "            # If mostly caps AND short, likely a title\n",
    "            if caps_ratio > 0.75 and len(words) <= 6:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def is_story_ending(text):\n",
    "    \"\"\"\n",
    "    Detect if a panel marks the END of a story.\n",
    "    The NEXT page would be a new story.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    text_upper = text.upper().strip()\n",
    "    \n",
    "    for pattern in STORY_END_PATTERNS:\n",
    "        if re.search(pattern, text_upper):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_story_boundaries(panels_df):\n",
    "    \"\"\"\n",
    "    Detect story boundaries within a single comic.\n",
    "    \n",
    "    Uses MULTIPLE SIGNALS (not just character names!):\n",
    "    1. Page number gaps > 1 (filtered ads between stories)\n",
    "    2. Title patterns in first panel of page (regex-based)\n",
    "    3. Story ending patterns (THE END, etc.)\n",
    "    4. Short ALL-CAPS text (classic title format)\n",
    "    \n",
    "    Args:\n",
    "        panels_df: DataFrame with panels from ONE comic, sorted by page/panel\n",
    "        \n",
    "    Returns:\n",
    "        list: List of page numbers where new stories begin\n",
    "    \"\"\"\n",
    "    if panels_df.empty:\n",
    "        return []\n",
    "    \n",
    "    boundaries = []\n",
    "    pages = sorted(panels_df['page_no'].unique())\n",
    "    \n",
    "    if len(pages) == 0:\n",
    "        return []\n",
    "    \n",
    "    # First valid page is always a boundary (story start)\n",
    "    boundaries.append(pages[0])\n",
    "    \n",
    "    # Track if previous page had a story ending\n",
    "    prev_page_had_ending = False\n",
    "    \n",
    "    for i in range(1, len(pages)):\n",
    "        current_page = pages[i]\n",
    "        previous_page = pages[i-1]\n",
    "        \n",
    "        # Signal 1: Page gap > 1 (ads were filtered between stories)\n",
    "        # This is the MOST RELIABLE signal!\n",
    "        if current_page - previous_page > 1:\n",
    "            boundaries.append(current_page)\n",
    "            prev_page_had_ending = False\n",
    "            continue\n",
    "        \n",
    "        # Signal 2: Previous page had \"THE END\" or similar\n",
    "        if prev_page_had_ending:\n",
    "            boundaries.append(current_page)\n",
    "            prev_page_had_ending = False\n",
    "            continue\n",
    "        \n",
    "        # Signal 3: Check first panel of this page for title patterns\n",
    "        page_panels = panels_df[panels_df['page_no'] == current_page]\n",
    "        if not page_panels.empty:\n",
    "            first_panel = page_panels.iloc[0]\n",
    "            text = str(first_panel.get('agg_text', ''))\n",
    "            \n",
    "            if is_story_title_panel(text):\n",
    "                # Don't add if it's right after another boundary (same story intro)\n",
    "                if boundaries and current_page - boundaries[-1] > 2:\n",
    "                    boundaries.append(current_page)\n",
    "        \n",
    "        # Check if THIS page has a story ending (for next iteration)\n",
    "        all_page_text = ' '.join(str(row.get('agg_text', '')) for _, row in page_panels.iterrows())\n",
    "        prev_page_had_ending = is_story_ending(all_page_text)\n",
    "    \n",
    "    return sorted(set(boundaries))\n",
    "\n",
    "\n",
    "# Test the detection\n",
    "print(\"Filter functions loaded!\")\n",
    "print(f\"  Title patterns: {len(TITLE_INDICATOR_PATTERNS)}\")\n",
    "print(f\"  Story end patterns: {len(STORY_END_PATTERNS)}\")\n",
    "print(f\"  Ad keywords: {len(AD_KEYWORDS)}\")\n",
    "\n",
    "# Test cases\n",
    "print(\"\\nTest cases:\")\n",
    "test_titles = [\n",
    "    \"BLUE BEETLE IN: THE MYSTERY CASE\",\n",
    "    \"CAPTAIN MARVEL VS THE VILLAIN\",\n",
    "    \"THE AMAZING SPIDER-MAN\",\n",
    "    \"CHAPTER 3: THE FINAL BATTLE\",\n",
    "    \"BY JACK KIRBY\",\n",
    "    \"HERCULES MODERN CHAMPION OF JUSTICE\",\n",
    "    \"SOME RANDOM DIALOGUE HERE\",  # Should be False\n",
    "]\n",
    "for t in test_titles:\n",
    "    result = is_story_title_panel(t)\n",
    "    print(f\"  '{t[:40]}...' -> {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell3-load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OCR data...\n",
      "\n",
      "============================================================\n",
      "RAW DATA STATISTICS\n",
      "============================================================\n",
      "Total panels: 420,000\n",
      "Total comics: 1,441\n",
      "Columns: ['comic_no', 'page_no', 'panel_no', 'img_path', 'agg_text', 'bubble_count', 'bubbles_json']\n",
      "\n",
      "Comic number range: 0 to 1447\n",
      "\n",
      "Comics to process: 1441\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 3: LOAD AND EXPLORE DATA\n",
    "=============================================================================\n",
    "Load the OCR CSV and get basic statistics.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading OCR data...\")\n",
    "df = pd.read_csv(OCR_CSV_PATH)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RAW DATA STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total panels: {len(df):,}\")\n",
    "print(f\"Total comics: {df['comic_no'].nunique():,}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Get list of all comics\n",
    "all_comics = sorted(df['comic_no'].unique())\n",
    "print(f\"\\nComic number range: {min(all_comics)} to {max(all_comics)}\")\n",
    "\n",
    "# Limit comics if testing\n",
    "if MAX_COMICS is not None:\n",
    "    all_comics = all_comics[:MAX_COMICS]\n",
    "    print(f\"\\n TESTING MODE: Limited to {MAX_COMICS} comics\")\n",
    "\n",
    "print(f\"\\nComics to process: {len(all_comics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell4-process-comics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comics...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1441/1441 [01:08<00:00, 21.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FILTERING STATISTICS\n",
      "============================================================\n",
      "Total panels (raw):        420,000\n",
      "Removed (cover pages):     2,667\n",
      "Removed (advertisements):  3,286\n",
      "Total stories detected:    11,514\n",
      "Stories skipped (short):   1,774\n",
      "Stories kept:              9,740\n",
      "Total panels (clean):      406,042\n",
      "\n",
      "Data reduction: 3.3%\n",
      "\n",
      "Average stories per comic: 6.8\n",
      "Average panels per story: 41.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 4: PROCESS ALL COMICS - SEGMENT INTO STORIES\n",
    "=============================================================================\n",
    "Apply all filters and segment each comic into separate stories.\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Processing comics...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Statistics tracking\n",
    "stats = {\n",
    "    'total_panels_raw': 0,\n",
    "    'panels_removed_cover': 0,\n",
    "    'panels_removed_ads': 0,\n",
    "    'total_stories': 0,\n",
    "    'stories_skipped_short': 0,\n",
    "    'total_panels_clean': 0,\n",
    "    'boundaries_by_page_gap': 0,\n",
    "    'boundaries_by_pattern': 0,\n",
    "}\n",
    "\n",
    "# Store all story segments\n",
    "all_story_segments = []  # List of (comic_no, story_idx, panels_list)\n",
    "\n",
    "for comic_no in tqdm(all_comics, desc=\"Processing comics\"):\n",
    "    # Get all panels for this comic\n",
    "    comic_df = df[df['comic_no'] == comic_no].copy()\n",
    "    comic_df = comic_df.sort_values(['page_no', 'panel_no'])\n",
    "    \n",
    "    stats['total_panels_raw'] += len(comic_df)\n",
    "    \n",
    "    # FILTER 1: Remove cover pages (page 0)\n",
    "    panels_before = len(comic_df)\n",
    "    comic_df = comic_df[comic_df['page_no'] >= MIN_PAGE_NUMBER]\n",
    "    stats['panels_removed_cover'] += (panels_before - len(comic_df))\n",
    "    \n",
    "    if comic_df.empty:\n",
    "        continue\n",
    "    \n",
    "    # FILTER 2: Remove advertisement panels\n",
    "    panels_before = len(comic_df)\n",
    "    comic_df = comic_df[~comic_df['agg_text'].apply(is_likely_advertisement)]\n",
    "    stats['panels_removed_ads'] += (panels_before - len(comic_df))\n",
    "    \n",
    "    if comic_df.empty:\n",
    "        continue\n",
    "    \n",
    "    # FILTER 3: Detect story boundaries\n",
    "    boundaries = detect_story_boundaries(comic_df)\n",
    "    \n",
    "    if not boundaries:\n",
    "        continue\n",
    "    \n",
    "    # Create story segments\n",
    "    for story_idx, start_page in enumerate(boundaries):\n",
    "        # Determine end page\n",
    "        if story_idx + 1 < len(boundaries):\n",
    "            end_page = boundaries[story_idx + 1] - 1\n",
    "        else:\n",
    "            end_page = comic_df['page_no'].max()\n",
    "        \n",
    "        # Get panels in this story segment\n",
    "        segment_df = comic_df[\n",
    "            (comic_df['page_no'] >= start_page) & \n",
    "            (comic_df['page_no'] <= end_page)\n",
    "        ]\n",
    "        \n",
    "        stats['total_stories'] += 1\n",
    "        \n",
    "        # FILTER 4: Skip short segments\n",
    "        if len(segment_df) < MIN_SEGMENT_LENGTH:\n",
    "            stats['stories_skipped_short'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Convert to list of panel dictionaries\n",
    "        panels_list = []\n",
    "        for _, row in segment_df.iterrows():\n",
    "            # Construct image path\n",
    "            img_path = IMAGES_DIR / str(comic_no) / f\"{int(row['page_no'])}_{int(row['panel_no'])}.jpg\"\n",
    "            \n",
    "            panels_list.append({\n",
    "                'comic_no': int(comic_no),\n",
    "                'page_no': int(row['page_no']),\n",
    "                'panel_no': int(row['panel_no']),\n",
    "                'image_path': str(img_path),\n",
    "                'text': str(row['agg_text']) if pd.notna(row['agg_text']) else '',\n",
    "            })\n",
    "        \n",
    "        stats['total_panels_clean'] += len(panels_list)\n",
    "        \n",
    "        all_story_segments.append({\n",
    "            'comic_no': comic_no,\n",
    "            'story_idx': story_idx,\n",
    "            'start_page': start_page,\n",
    "            'end_page': end_page,\n",
    "            'panels': panels_list,\n",
    "        })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FILTERING STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total panels (raw):        {stats['total_panels_raw']:,}\")\n",
    "print(f\"Removed (cover pages):     {stats['panels_removed_cover']:,}\")\n",
    "print(f\"Removed (advertisements):  {stats['panels_removed_ads']:,}\")\n",
    "print(f\"Total stories detected:    {stats['total_stories']:,}\")\n",
    "print(f\"Stories skipped (short):   {stats['stories_skipped_short']:,}\")\n",
    "print(f\"Stories kept:              {len(all_story_segments):,}\")\n",
    "print(f\"Total panels (clean):      {stats['total_panels_clean']:,}\")\n",
    "print(f\"\\nData reduction: {100*(1 - stats['total_panels_clean']/stats['total_panels_raw']):.1f}%\")\n",
    "print(f\"\\nAverage stories per comic: {len(all_story_segments)/len(all_comics):.1f}\")\n",
    "print(f\"Average panels per story: {stats['total_panels_clean']/len(all_story_segments):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell5-build-sequences",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sequences from story segments...\n",
      "============================================================\n",
      "\n",
      "Step 1: Scanning existing images (one-time cost)...\n",
      "  Found 1441 comic folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1441/1441 [00:04<00:00, 320.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total images found: 879,226\n",
      "\n",
      "Step 2: Building sequences (with fast image verification)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sequences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9740/9740 [00:02<00:00, 3932.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SEQUENCE STATISTICS\n",
      "============================================================\n",
      "Total sequences built: 357,342\n",
      "Skipped (missing images): 0\n",
      "Average sequences per story: 36.7\n",
      "\n",
      "============================================================\n",
      "SAMPLE SEQUENCE\n",
      "============================================================\n",
      "Comic: 0, Story: 0\n",
      "\n",
      "Context panels:\n",
      "  1. Page 2, Panel 0: CK HIM ,PLAS! CK HIM WN! PLASTIC MAN\n",
      "  2. Page 2, Panel 1: PLASTICMAN IN HIS CAREER AS AS CRIME-FIGHTER, PLAS...\n",
      "  3. Page 3, Panel 0: NOW IF I PUT THIS HERE... AND THIS ONE HERE...\n",
      "  4. Page 3, Panel 1: AND ADJUST THIS SPRING...\n",
      "  5. Page 3, Panel 2: THAT DOES IT! IT'S FINISHED AND READY FOR THE FINA...\n",
      "\n",
      "Target panel:\n",
      "  Page 3, Panel 3\n",
      "  Text: I'VE BEEN WORKING ON IT FOR DAYS! I MUST GET A BREATH OF AIR AND CLEAR MY HEAD BEFORE I BEGIN MY TES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 5: BUILD TRAINING SEQUENCES (OPTIMIZED)\n",
    "=============================================================================\n",
    "OPTIMIZATION: Pre-cache all existing image paths into a set for instant lookup.\n",
    "This reduces ~2 million disk checks to ~420K (done once), speeding up from \n",
    "~1.5 hours to ~5 minutes.\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"Building sequences from story segments...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# =========================================================================\n",
    "# OPTIMIZATION: Pre-scan all existing images into a set\n",
    "# =========================================================================\n",
    "print(\"\\nStep 1: Scanning existing images (one-time cost)...\")\n",
    "\n",
    "existing_images = set()\n",
    "\n",
    "# Count total comics for progress bar\n",
    "comic_folders = [f for f in IMAGES_DIR.iterdir() if f.is_dir()]\n",
    "print(f\"  Found {len(comic_folders)} comic folders\")\n",
    "\n",
    "for comic_folder in tqdm(comic_folders, desc=\"Scanning images\"):\n",
    "    for img_file in comic_folder.iterdir():\n",
    "        if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            existing_images.add(str(img_file))\n",
    "\n",
    "print(f\"  Total images found: {len(existing_images):,}\")\n",
    "\n",
    "# =========================================================================\n",
    "# Build sequences with FAST set lookup\n",
    "# =========================================================================\n",
    "print(\"\\nStep 2: Building sequences (with fast image verification)...\")\n",
    "\n",
    "all_sequences = []\n",
    "skipped_no_images = 0\n",
    "\n",
    "for segment in tqdm(all_story_segments, desc=\"Building sequences\"):\n",
    "    panels = segment['panels']\n",
    "    \n",
    "    # Need at least CONTEXT_WINDOW + 1 panels to make a sequence\n",
    "    if len(panels) <= CONTEXT_WINDOW:\n",
    "        continue\n",
    "    \n",
    "    # Create sequences within this story segment\n",
    "    for i in range(CONTEXT_WINDOW, len(panels)):\n",
    "        context_panels = panels[i - CONTEXT_WINDOW : i]\n",
    "        target_panel = panels[i]\n",
    "        \n",
    "        # FAST: Check against pre-cached set (instant lookup!)\n",
    "        all_exist = True\n",
    "        for p in context_panels + [target_panel]:\n",
    "            if p['image_path'] not in existing_images:\n",
    "                all_exist = False\n",
    "                break\n",
    "        \n",
    "        if not all_exist:\n",
    "            skipped_no_images += 1\n",
    "            continue\n",
    "        \n",
    "        # Build the sequence\n",
    "        sequence = {\n",
    "            'comic_no': segment['comic_no'],\n",
    "            'story_idx': segment['story_idx'],\n",
    "            'context': context_panels,\n",
    "            'target': target_panel,\n",
    "            'target_text': target_panel['text'],\n",
    "            'context_texts': [p['text'] for p in context_panels],\n",
    "        }\n",
    "        \n",
    "        all_sequences.append(sequence)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SEQUENCE STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total sequences built: {len(all_sequences):,}\")\n",
    "print(f\"Skipped (missing images): {skipped_no_images:,}\")\n",
    "if all_story_segments:\n",
    "    print(f\"Average sequences per story: {len(all_sequences) / len(all_story_segments):.1f}\")\n",
    "\n",
    "# Show sample sequence\n",
    "if all_sequences:\n",
    "    sample = all_sequences[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAMPLE SEQUENCE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Comic: {sample['comic_no']}, Story: {sample['story_idx']}\")\n",
    "    print(f\"\\nContext panels:\")\n",
    "    for i, ctx in enumerate(sample['context']):\n",
    "        text_preview = ctx['text'][:50] + '...' if len(ctx['text']) > 50 else ctx['text']\n",
    "        print(f\"  {i+1}. Page {ctx['page_no']}, Panel {ctx['panel_no']}: {text_preview}\")\n",
    "    print(f\"\\nTarget panel:\")\n",
    "    print(f\"  Page {sample['target']['page_no']}, Panel {sample['target']['panel_no']}\")\n",
    "    target_preview = sample['target_text'][:100] + '...' if len(sample['target_text']) > 100 else sample['target_text']\n",
    "    print(f\"  Text: {target_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d4db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMIC #0 ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Number of stories detected: 7\n",
      "\n",
      "--- Story 1 ---\n",
      "  Pages: 2 to 8\n",
      "  Panels: 44\n",
      "  First panel text: \"CK HIM ,PLAS! CK HIM WN! PLASTIC MAN...\"\n",
      "\n",
      "--- Story 2 ---\n",
      "  Pages: 9 to 17\n",
      "  Panels: 56\n",
      "  First panel text: \"NOW FIRST I REMOVE THESE!...\"\n",
      "\n",
      "--- Story 3 ---\n",
      "  Pages: 18 to 25\n",
      "  Panels: 52\n",
      "  First panel text: \"I DON'T KNOW! I MUSTA BURNED IT SOMEWHERE! THAT'S NO ORDINARY BURN! IT LOOKS TO ...\"\n",
      "\n",
      "--- Story 4 ---\n",
      "  Pages: 26 to 29\n",
      "  Panels: 28\n",
      "  First panel text: \"WELL, CAN YOU? AWRK!...\"\n",
      "\n",
      "--- Story 5 ---\n",
      "  Pages: 30 to 39\n",
      "  Panels: 58\n",
      "  First panel text: \"PLASTIC SHE'S A FAST SLOOP! WE'LL BE AT AMOS ISLAND BY MORNING! IT'S ALMOST DARK...\"\n",
      "\n",
      "--- Story 6 ---\n",
      "  Pages: 40 to 42\n",
      "  Panels: 11\n",
      "  First panel text: \"...\"\n",
      "\n",
      "--- Story 7 ---\n",
      "  Pages: 43 to 48\n",
      "  Panels: 41\n",
      "  First panel text: \"STILL TRYING, EH ROCKY?...\"\n",
      "\n",
      "============================================================\n",
      "\n",
      "BASIC INFO:\n",
      "  Total panels (after cover removal): 293\n",
      "  Page range: 2 to 49\n",
      "\n",
      "BOUNDARIES DETECTED:\n",
      "  Boundary pages: [np.int64(2), np.int64(9), np.int64(18), np.int64(26), np.int64(30), np.int64(40), np.int64(43), np.int64(49)]\n",
      "  Number of boundaries: 8\n",
      "\n",
      "TEXT AT EACH BOUNDARY:\n",
      "  Page   2: \"CK HIM ,PLAS! CK HIM WN! PLASTIC MAN...\"\n",
      "  Page   9: \"NOW FIRST I REMOVE THESE!...\"\n",
      "  Page  18: \"I DON'T KNOW! I MUSTA BURNED IT SOMEWHERE! THAT'S NO ORDINARY BURN! IT...\"\n",
      "  Page  26: \"WELL, CAN YOU? AWRK!...\"\n",
      "  Page  30: \"PLASTIC SHE'S A FAST SLOOP! WE'LL BE AT AMOS ISLAND BY MORNING! IT'S A...\"\n",
      "  Page  40: \"nan...\"\n",
      "  Page  43: \"STILL TRYING, EH ROCKY?...\"\n",
      "  Page  49: \"EXPLOITS AGAINST CRIME! Meet KEN SHANNON TOUGHEST PRIVATE EYE OF THEM ...\"\n",
      "\n",
      "PAGE GAPS DETECTED:\n",
      "  Gap: Page 38 â†’ Page 40 (missing: [39])\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DIAGNOSTIC: Check how Comic #0 was segmented\n",
    "Run this cell to inspect Comic #0 specifically\n",
    "\"\"\"\n",
    "\n",
    "# Find all story segments for Comic 0\n",
    "comic_0_segments = [seg for seg in all_story_segments if seg['comic_no'] == 0]\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"COMIC #0 ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nNumber of stories detected: {len(comic_0_segments)}\")\n",
    "\n",
    "for i, seg in enumerate(comic_0_segments):\n",
    "    print(f\"\\n--- Story {i+1} ---\")\n",
    "    print(f\"  Pages: {seg['start_page']} to {seg['end_page']}\")\n",
    "    print(f\"  Panels: {len(seg['panels'])}\")\n",
    "    \n",
    "    # Show first panel text (what triggered the boundary?)\n",
    "    if seg['panels']:\n",
    "        first_text = seg['panels'][0]['text'][:80]\n",
    "        print(f\"  First panel text: \\\"{first_text}...\\\"\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Also check: What pages in Comic 0 were detected as boundaries?\n",
    "comic_0_df = df[df['comic_no'] == 0].copy()\n",
    "comic_0_df = comic_0_df[comic_0_df['page_no'] >= 1]  # Exclude cover\n",
    "comic_0_df = comic_0_df.sort_values(['page_no', 'panel_no'])\n",
    "\n",
    "print(f\"\\nBASIC INFO:\")\n",
    "print(f\"  Total panels (after cover removal): {len(comic_0_df)}\")\n",
    "print(f\"  Page range: {comic_0_df['page_no'].min()} to {comic_0_df['page_no'].max()}\")\n",
    "\n",
    "print(f\"\\nBOUNDARIES DETECTED:\")\n",
    "boundaries = detect_story_boundaries(comic_0_df)\n",
    "print(f\"  Boundary pages: {boundaries}\")\n",
    "print(f\"  Number of boundaries: {len(boundaries)}\")\n",
    "\n",
    "# Show what text triggered each boundary\n",
    "print(f\"\\nTEXT AT EACH BOUNDARY:\")\n",
    "for page in boundaries:\n",
    "    page_panels = comic_0_df[comic_0_df['page_no'] == page]\n",
    "    if not page_panels.empty:\n",
    "        first_text = str(page_panels.iloc[0]['agg_text'])[:70]\n",
    "        print(f\"  Page {page:3d}: \\\"{first_text}...\\\"\")\n",
    "\n",
    "# Show page gaps (if any)\n",
    "print(f\"\\nPAGE GAPS DETECTED:\")\n",
    "pages = sorted(comic_0_df['page_no'].unique())\n",
    "gaps = []\n",
    "for i in range(1, len(pages)):\n",
    "    if pages[i] - pages[i-1] > 1:\n",
    "        gaps.append((pages[i-1], pages[i]))\n",
    "        \n",
    "if gaps:\n",
    "    for start, end in gaps:\n",
    "        print(f\"  Gap: Page {start} â†’ Page {end} (missing: {list(range(start+1, end))})\")\n",
    "else:\n",
    "    print(f\"  No page gaps found (all pages consecutive)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell6-train-test-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/val/test split...\n",
      "============================================================\n",
      "\n",
      "COMIC-LEVEL SPLIT:\n",
      "  Train comics: 1,007 (70.0%)\n",
      "  Val comics:   215 (14.9%)\n",
      "  Test comics:  217 (15.1%)\n",
      "\n",
      "SEQUENCE-LEVEL SPLIT:\n",
      "  Train sequences: 249,576 (69.8%)\n",
      "  Val sequences:   53,236 (14.9%)\n",
      "  Test sequences:  54,530 (15.3%)\n",
      "\n",
      " Split info saved to: /scratch/bftl/hsekar/comics_project/data/processed/clean_split_info.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 6: CREATE TRAIN/VAL/TEST SPLIT\n",
    "=============================================================================\n",
    "Split at the COMIC level (not sequence level) to ensure:\n",
    "- No story from a comic appears in both train and test\n",
    "- Proper generalization testing\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "print(\"Creating train/val/test split...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Get unique comics that have sequences\n",
    "comics_with_sequences = list(set(seq['comic_no'] for seq in all_sequences))\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(comics_with_sequences)\n",
    "\n",
    "# Calculate split sizes\n",
    "n_total = len(comics_with_sequences)\n",
    "n_train = int(n_total * TRAIN_RATIO)\n",
    "n_val = int(n_total * VAL_RATIO)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "# Split comics\n",
    "train_comics = set(comics_with_sequences[:n_train])\n",
    "val_comics = set(comics_with_sequences[n_train:n_train + n_val])\n",
    "test_comics = set(comics_with_sequences[n_train + n_val:])\n",
    "\n",
    "# Split sequences based on comic assignment\n",
    "train_sequences = [seq for seq in all_sequences if seq['comic_no'] in train_comics]\n",
    "val_sequences = [seq for seq in all_sequences if seq['comic_no'] in val_comics]\n",
    "test_sequences = [seq for seq in all_sequences if seq['comic_no'] in test_comics]\n",
    "\n",
    "print(f\"\\nCOMIC-LEVEL SPLIT:\")\n",
    "print(f\"  Train comics: {len(train_comics):,} ({100*len(train_comics)/n_total:.1f}%)\")\n",
    "print(f\"  Val comics:   {len(val_comics):,} ({100*len(val_comics)/n_total:.1f}%)\")\n",
    "print(f\"  Test comics:  {len(test_comics):,} ({100*len(test_comics)/n_total:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSEQUENCE-LEVEL SPLIT:\")\n",
    "print(f\"  Train sequences: {len(train_sequences):,} ({100*len(train_sequences)/len(all_sequences):.1f}%)\")\n",
    "print(f\"  Val sequences:   {len(val_sequences):,} ({100*len(val_sequences)/len(all_sequences):.1f}%)\")\n",
    "print(f\"  Test sequences:  {len(test_sequences):,} ({100*len(test_sequences)/len(all_sequences):.1f}%)\")\n",
    "\n",
    "# Save split info\n",
    "split_info = {\n",
    "    'train_comics': sorted(list(train_comics)),\n",
    "    'val_comics': sorted(list(val_comics)),\n",
    "    'test_comics': sorted(list(test_comics)),\n",
    "    'n_train_sequences': len(train_sequences),\n",
    "    'n_val_sequences': len(val_sequences),\n",
    "    'n_test_sequences': len(test_sequences),\n",
    "    'context_window': CONTEXT_WINDOW,\n",
    "    'random_seed': RANDOM_SEED,\n",
    "}\n",
    "\n",
    "split_info_path = PROCESSED_DIR / \"clean_split_info.json\"\n",
    "\n",
    "# Convert numpy types to native Python types for JSON serialization\n",
    "def convert_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert split_info before saving\n",
    "split_info = convert_to_native(split_info)\n",
    "\n",
    "# Now save\n",
    "with open(split_info_path, 'w') as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "\n",
    "print(f\"\\n Split info saved to: {split_info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell7-save-sequences",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed sequences...\n",
      "============================================================\n",
      "Train sequences saved: /scratch/bftl/hsekar/comics_project/data/processed/train_sequences.pkl\n",
      "Val sequences saved: /scratch/bftl/hsekar/comics_project/data/processed/val_sequences.pkl\n",
      "Test sequences saved: /scratch/bftl/hsekar/comics_project/data/processed/test_sequences.pkl\n",
      "Sample sequences saved: /scratch/bftl/hsekar/comics_project/data/processed/sample_sequences.json\n",
      "\n",
      "============================================================\n",
      "FILES SAVED\n",
      "============================================================\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/train_sequences.pkl\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/val_sequences.pkl\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/test_sequences.pkl\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/clean_split_info.json\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/sample_sequences.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 7: SAVE PROCESSED SEQUENCES\n",
    "=============================================================================\n",
    "Save all sequences in formats ready for:\n",
    "1. Zero-shot evaluation\n",
    "2. Fine-tuning\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"Saving processed sequences...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save as pickle (fastest for Python)\n",
    "train_path = PROCESSED_DIR / \"train_sequences.pkl\"\n",
    "val_path = PROCESSED_DIR / \"val_sequences.pkl\"\n",
    "test_path = PROCESSED_DIR / \"test_sequences.pkl\"\n",
    "\n",
    "with open(train_path, 'wb') as f:\n",
    "    pickle.dump(train_sequences, f)\n",
    "print(f\"Train sequences saved: {train_path}\")\n",
    "\n",
    "with open(val_path, 'wb') as f:\n",
    "    pickle.dump(val_sequences, f)\n",
    "print(f\"Val sequences saved: {val_path}\")\n",
    "\n",
    "with open(test_path, 'wb') as f:\n",
    "    pickle.dump(test_sequences, f)\n",
    "print(f\"Test sequences saved: {test_path}\")\n",
    "\n",
    "# Also save a small sample as JSON for easy inspection\n",
    "sample_sequences = {\n",
    "    'train_sample': train_sequences[:5] if len(train_sequences) >= 5 else train_sequences,\n",
    "    'val_sample': val_sequences[:5] if len(val_sequences) >= 5 else val_sequences,\n",
    "    'test_sample': test_sequences[:5] if len(test_sequences) >= 5 else test_sequences,\n",
    "}\n",
    "\n",
    "sample_path = PROCESSED_DIR / \"sample_sequences.json\"\n",
    "with open(sample_path, 'w') as f:\n",
    "    json.dump(sample_sequences, f, indent=2, default=str)\n",
    "print(f\"Sample sequences saved: {sample_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FILES SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  {train_path}\")\n",
    "print(f\"  {val_path}\")\n",
    "print(f\"  {test_path}\")\n",
    "print(f\"  {split_info_path}\")\n",
    "print(f\"  {sample_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell8-create-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROMPT STYLE 1: HYBRID (Images + OCR hints)\n",
      "======================================================================\n",
      "You are reading a comic book.\n",
      "I will show you the previous panels as images.\n",
      "Based on them, write the dialogue and narration that should appear in the NEXT panel of the story.\n",
      "Keep it concise and in a natural comic-book style.\n",
      "\n",
      "Note: Here is some text detected in the panels (may be incomplete or noisy):\n",
      "  Panel 1: \"RANG-A-TANG THE WONDER DOG! BECAUSE OF HIS ALMOST-HUMAN BRAIN...\"\n",
      "  Panel 2: \"I ASKED YOU TO COME HERE BECAUSE I WANTED TO SIGN THIS FAMOUS DOG!\"\n",
      "  Panel 4: \"WE'RE ABOUT TO GO BANKRUPT!\"\n",
      "  Panel 5: \"THAT'S RIGHT! NAWSON SWELLES, HIS NAME IS!\"\n",
      "\n",
      "Use this as a hint, but trust what you SEE in the images as the primary source.\n",
      "\n",
      "======================================================================\n",
      "PROMPT STYLE 2: SIMPLE (Images only, matches Ollama)\n",
      "======================================================================\n",
      "You are reading a comic book.\n",
      "I will show you the previous panels as images.\n",
      "Based on them, write the dialogue and narration that should appear in the NEXT panel of the story.\n",
      "Keep it concise and in a natural comic-book style.\n",
      "\n",
      "======================================================================\n",
      "HYBRID with empty/noisy OCR (falls back to simple)\n",
      "======================================================================\n",
      "You are reading a comic book.\n",
      "I will show you the previous panels as images.\n",
      "Based on them, write the dialogue and narration that should appear in the NEXT panel of the story.\n",
      "Keep it concise and in a natural comic-book style.\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATION\n",
      "======================================================================\n",
      "\n",
      "For zero-shot evaluation:\n",
      "  - Use create_prompt_hybrid() to leverage OCR when available\n",
      "  - This reduces hallucination while keeping images as primary source\n",
      "\n",
      "For direct Ollama comparison:\n",
      "  - Use create_prompt_simple() for apples-to-apples comparison\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 8: CREATE HYBRID PROMPT (Images Primary, OCR as Support)\n",
    "=============================================================================\n",
    "Images are the primary source of truth. OCR text is used as a helpful hint\n",
    "when available and meaningful, but the model is instructed to trust the images.\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt_hybrid(context_texts=None):\n",
    "    \"\"\"\n",
    "    Hybrid prompt: Images are primary, OCR provides hints when available.\n",
    "    \n",
    "    Args:\n",
    "        context_texts: List of OCR texts from context panels (can be None or empty)\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    base_prompt = (\n",
    "        \"You are reading a comic book.\\n\"\n",
    "        \"I will show you the previous panels as images.\\n\"\n",
    "        \"Based on them, write the dialogue and narration that should appear \"\n",
    "        \"in the NEXT panel of the story.\\n\"\n",
    "        \"Keep it concise and in a natural comic-book style.\"\n",
    "    )\n",
    "    \n",
    "    # Check if we have any meaningful OCR text\n",
    "    has_useful_ocr = False\n",
    "    if context_texts:\n",
    "        # Filter out empty, very short, or noisy OCR\n",
    "        useful_texts = []\n",
    "        for i, text in enumerate(context_texts, 1):\n",
    "            text = str(text).strip() if text else \"\"\n",
    "            # Only include if text is meaningful (>10 chars, not just punctuation)\n",
    "            if len(text) > 10 and any(c.isalpha() for c in text):\n",
    "                useful_texts.append((i, text))\n",
    "        has_useful_ocr = len(useful_texts) > 0\n",
    "    \n",
    "    if has_useful_ocr:\n",
    "        # Add OCR as supplementary context\n",
    "        ocr_hint = \"\\n\\nNote: Here is some text detected in the panels (may be incomplete or noisy):\\n\"\n",
    "        for panel_num, text in useful_texts:\n",
    "            # Truncate very long OCR to avoid overwhelming the prompt\n",
    "            truncated = text[:150] + \"...\" if len(text) > 150 else text\n",
    "            ocr_hint += f\"  Panel {panel_num}: \\\"{truncated}\\\"\\n\"\n",
    "        ocr_hint += \"\\nUse this as a hint, but trust what you SEE in the images as the primary source.\"\n",
    "        \n",
    "        return base_prompt + ocr_hint\n",
    "    else:\n",
    "        return base_prompt\n",
    "\n",
    "\n",
    "def create_prompt_simple():\n",
    "    \"\"\"\n",
    "    Simple prompt matching Ollama baseline (no OCR).\n",
    "    Use this for direct comparison with Ollama results.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"You are reading a comic book.\\n\"\n",
    "        \"I will show you the previous panels as images.\\n\"\n",
    "        \"Based on them, write the dialogue and narration that should appear \"\n",
    "        \"in the NEXT panel of the story.\\n\"\n",
    "        \"Keep it concise and in a natural comic-book style.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMO: Show both prompt styles\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROMPT STYLE 1: HYBRID (Images + OCR hints)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_texts = [\n",
    "    \"RANG-A-TANG THE WONDER DOG! BECAUSE OF HIS ALMOST-HUMAN BRAIN...\",\n",
    "    \"I ASKED YOU TO COME HERE BECAUSE I WANTED TO SIGN THIS FAMOUS DOG!\",\n",
    "    \"\",  # Empty panel (action only)\n",
    "    \"WE'RE ABOUT TO GO BANKRUPT!\",\n",
    "    \"THAT'S RIGHT! NAWSON SWELLES, HIS NAME IS!\",\n",
    "]\n",
    "\n",
    "print(create_prompt_hybrid(sample_texts))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROMPT STYLE 2: SIMPLE (Images only, matches Ollama)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(create_prompt_simple())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID with empty/noisy OCR (falls back to simple)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "noisy_texts = [\"\", \"...\", \"nan\", \"!?\", \"  \"]\n",
    "print(create_prompt_hybrid(noisy_texts))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "For zero-shot evaluation:\n",
    "  - Use create_prompt_hybrid() to leverage OCR when available\n",
    "  - This reduces hallucination while keeping images as primary source\n",
    "\n",
    "For direct Ollama comparison:\n",
    "  - Use create_prompt_simple() for apples-to-apples comparison\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell9-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    DATA CLEANING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "WHAT WAS DONE:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Removed cover pages (page 0)\n",
      "Filtered advertisement panels (3,286 removed)\n",
      "Detected story boundaries using PATTERNS (not just character names!):\n",
      "   - Page gaps (ads filtered between stories)\n",
      "   - Title patterns (\"IN:\", \"STARRING\", \"CHAPTER\", etc.)\n",
      "   - Story endings (\"THE END\", \"TO BE CONTINUED\")\n",
      "   - Short ALL-CAPS text\n",
      "Skipped short segments (1,774 skipped)\n",
      "Built clean sequences (357,342 total)\n",
      "Created train/val/test split at comic level\n",
      "Saved processed data to /scratch/bftl/hsekar/comics_project/data/processed\n",
      "\n",
      "DATA SUMMARY:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Raw panels:           420,000\n",
      "  Clean panels:         406,042\n",
      "  Total stories:        9,740\n",
      "  Total sequences:      357,342\n",
      "  \n",
      "  Train sequences:      249,576\n",
      "  Validation sequences: 53,236\n",
      "  Test sequences:       54,530\n",
      "\n",
      "FILES CREATED:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/train_sequences.pkl\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/val_sequences.pkl\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/test_sequences.pkl\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/clean_split_info.json\n",
      "  /scratch/bftl/hsekar/comics_project/data/processed/sample_sequences.json\n",
      "\n",
      "NEXT STEPS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Run zero-shot evaluation with the cleaned test set\n",
      "2. Fine-tune LLaVA on the cleaned training set\n",
      "3. Compare zero-shot vs fine-tuned results\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 9: FINAL SUMMARY\n",
    "=============================================================================\n",
    "Summary of what was accomplished and next steps.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"                    DATA CLEANING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "WHAT WAS DONE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Removed cover pages (page 0)\n",
    "Filtered advertisement panels ({stats['panels_removed_ads']:,} removed)\n",
    "Detected story boundaries using PATTERNS (not just character names!):\n",
    "   - Page gaps (ads filtered between stories)\n",
    "   - Title patterns (\"IN:\", \"STARRING\", \"CHAPTER\", etc.)\n",
    "   - Story endings (\"THE END\", \"TO BE CONTINUED\")\n",
    "   - Short ALL-CAPS text\n",
    "Skipped short segments ({stats['stories_skipped_short']:,} skipped)\n",
    "Built clean sequences ({len(all_sequences):,} total)\n",
    "Created train/val/test split at comic level\n",
    "Saved processed data to {PROCESSED_DIR}\n",
    "\n",
    "DATA SUMMARY:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Raw panels:           {stats['total_panels_raw']:,}\n",
    "  Clean panels:         {stats['total_panels_clean']:,}\n",
    "  Total stories:        {len(all_story_segments):,}\n",
    "  Total sequences:      {len(all_sequences):,}\n",
    "  \n",
    "  Train sequences:      {len(train_sequences):,}\n",
    "  Validation sequences: {len(val_sequences):,}\n",
    "  Test sequences:       {len(test_sequences):,}\n",
    "\n",
    "FILES CREATED:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  {PROCESSED_DIR}/train_sequences.pkl\n",
    "  {PROCESSED_DIR}/val_sequences.pkl\n",
    "  {PROCESSED_DIR}/test_sequences.pkl\n",
    "  {PROCESSED_DIR}/clean_split_info.json\n",
    "  {PROCESSED_DIR}/sample_sequences.json\n",
    "\n",
    "NEXT STEPS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Run zero-shot evaluation with the cleaned test set\n",
    "2. Fine-tune LLaVA on the cleaned training set\n",
    "3. Compare zero-shot vs fine-tuned results\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "675c2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script saved to: /scratch/bftl/hsekar/comics_project/scripts/zero_shot_eval.py\n",
      "\n",
      "Changes made:\n",
      "  - MAX_NEW_TOKENS: 256 â†’ 512\n",
      "  - Prompt: asks for CHARACTERS, SETTING, ACTION, DIALOGUE\n",
      "  - Output files: zero_shot_100seq_detailed.pkl/json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL: Create zero_shot_eval.py with detailed scene description\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "script_content = '''#!/usr/bin/env python3\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = Path(\"/scratch/bftl/hsekar/comics_project/data/processed\")\n",
    "OUTPUT_DIR = Path(\"/scratch/bftl/hsekar/comics_project/outputs/zero_shot\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_SEQUENCES = 100\n",
    "RANDOM_SEED = 42\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "MAX_NEW_TOKENS = 512  # Increased for detailed output\n",
    "\n",
    "print(f\"Sequences: {NUM_SEQUENCES}\")\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Max tokens: {MAX_NEW_TOKENS}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\\\nLoading model...\")\n",
    "processor = LlavaNextProcessor.from_pretrained(MODEL_ID)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\", \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\\\nLoading test sequences...\")\n",
    "with open(PROCESSED_DIR / \"test_sequences.pkl\", \"rb\") as f:\n",
    "    test_sequences = pickle.load(f)\n",
    "print(f\"Total: {len(test_sequences)}\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "sampled = random.sample(test_sequences, min(NUM_SEQUENCES, len(test_sequences)))\n",
    "print(f\"Sampled: {len(sampled)}\")\n",
    "\n",
    "# Prompt function - DETAILED VERSION\n",
    "def create_prompt(context_texts):\n",
    "    base = \"\"\"You are reading a comic book. I will show you the previous panels as images.\n",
    "\n",
    "Based on them, predict what happens in the NEXT panel. Provide a detailed description:\n",
    "\n",
    "1. CHARACTERS: Who is present? Describe their appearance, expressions, and emotions.\n",
    "\n",
    "2. SETTING: Where is this happening? Describe the background, environment, and any visible objects.\n",
    "\n",
    "3. ACTION: What are the characters doing? Describe their poses, movements, and interactions.\n",
    "\n",
    "4. DIALOGUE: Write any speech bubbles or narration boxes that would appear.\n",
    "\n",
    "Format your response exactly like this:\n",
    "CHARACTERS: [Detailed character descriptions]\n",
    "SETTING: [Detailed environment description]\n",
    "ACTION: [What is happening in the scene]\n",
    "DIALOGUE: [Speech bubbles and narration text]\"\"\"\n",
    "    \n",
    "    # Add OCR hints if available\n",
    "    useful = []\n",
    "    for i, t in enumerate(context_texts, 1):\n",
    "        t = str(t).strip() if t else \"\"\n",
    "        if len(t) > 10 and any(c.isalpha() for c in t):\n",
    "            useful.append((i, t))\n",
    "    \n",
    "    if useful:\n",
    "        hint = \"\\\\n\\\\nText detected in previous panels:\\\\n\"\n",
    "        for num, txt in useful:\n",
    "            truncated = txt[:150] + \"...\" if len(txt) > 150 else txt\n",
    "            hint += f\"  Panel {num}: \\\\\"{truncated}\\\\\"\\\\n\"\n",
    "        return base + hint\n",
    "    return base\n",
    "\n",
    "# Prediction function\n",
    "def predict(image_paths, context_texts):\n",
    "    # Load images\n",
    "    images = []\n",
    "    for p in image_paths:\n",
    "        try:\n",
    "            images.append(Image.open(p).convert(\"RGB\"))\n",
    "        except Exception as e:\n",
    "            print(f\"    Could not load image: {p}\")\n",
    "            images.append(Image.new(\"RGB\", (224, 224), (128, 128, 128)))\n",
    "    \n",
    "    prompt = create_prompt(context_texts)\n",
    "    \n",
    "    # Format for LLaVA 1.6 Mistral\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *[{\"type\": \"image\"} for _ in images],\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(text=text_prompt, images=images, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=MAX_NEW_TOKENS, \n",
    "            do_sample=False,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"[/INST]\" in full_text:\n",
    "        return full_text.split(\"[/INST]\")[-1].strip()\n",
    "    return full_text.strip()\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\\\nRunning evaluation...\")\n",
    "results = []\n",
    "start = time.time()\n",
    "\n",
    "for idx, seq in enumerate(sampled):\n",
    "    if idx % 10 == 0:\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Progress: {idx+1}/{len(sampled)} | Elapsed: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Get data\n",
    "    context = seq.get('context', [])\n",
    "    image_paths = [panel.get('image_path', '') for panel in context]\n",
    "    context_texts = seq.get('context_texts', [])\n",
    "    target = seq.get('target', {})\n",
    "    ground_truth = seq.get('target_text', '')\n",
    "    \n",
    "    try:\n",
    "        pred = predict(image_paths, context_texts)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error on seq {idx}: {e}\")\n",
    "        pred = f\"[ERROR: {e}]\"\n",
    "    \n",
    "    results.append({\n",
    "        'idx': idx,\n",
    "        'comic_no': int(seq.get('comic_no', 0)),\n",
    "        'story_idx': seq.get('story_idx'),\n",
    "        'context_texts': context_texts,\n",
    "        'ground_truth': ground_truth,\n",
    "        'prediction': pred,\n",
    "        'target_path': target.get('image_path', '')\n",
    "    })\n",
    "\n",
    "total_time = time.time() - start\n",
    "print(f\"\\\\nDone! Total time: {total_time:.1f}s ({total_time/len(sampled):.2f}s per sequence)\")\n",
    "\n",
    "# Save results\n",
    "pkl_path = OUTPUT_DIR / f\"zero_shot_{NUM_SEQUENCES}seq_detailed.pkl\"\n",
    "json_path = OUTPUT_DIR / f\"zero_shot_{NUM_SEQUENCES}seq_detailed.json\"\n",
    "\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"\\\\nSaved: {pkl_path}\")\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Saved: {json_path}\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for r in results[:3]:\n",
    "    print(f\"\\\\n{'â”€'*70}\")\n",
    "    print(f\"Comic {r['comic_no']}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(f\"\\\\nGround Truth OCR:\")\n",
    "    print(f\"  {r['ground_truth'][:150] if r['ground_truth'] else '[Empty]'}...\")\n",
    "    print(f\"\\\\nPrediction:\")\n",
    "    print(r['prediction'][:500])\n",
    "'''\n",
    "\n",
    "script_path = Path(\"/scratch/bftl/hsekar/comics_project/scripts/zero_shot_eval.py\")\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(f\"Script saved to: {script_path}\")\n",
    "print(f\"\\nChanges made:\")\n",
    "print(f\"  - MAX_NEW_TOKENS: 256 â†’ 512\")\n",
    "print(f\"  - Prompt: asks for CHARACTERS, SETTING, ACTION, DIALOGUE\")\n",
    "print(f\"  - Output files: zero_shot_100seq_detailed.pkl/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "200f0cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM script saved to: /scratch/bftl/hsekar/comics_project/scripts/zero_shot_eval.sbatch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL: Create FIXED zero_shot_eval.sbatch\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "sbatch_content = '''#!/bin/bash\n",
    "#SBATCH --job-name=zero_shot\n",
    "#SBATCH --account=bftl-delta-gpu\n",
    "#SBATCH --partition=gpuA100x4\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-node=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=64G\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH --output=/scratch/bftl/hsekar/comics_project/logs/zero_shot_%j.out\n",
    "#SBATCH --error=/scratch/bftl/hsekar/comics_project/logs/zero_shot_%j.err\n",
    "\n",
    "echo \"Job started: $(date)\"\n",
    "echo \"Node: $SLURM_NODELIST\"\n",
    "\n",
    "# Load conda\n",
    "source /sw/external/python/anaconda3/etc/profile.d/conda.sh\n",
    "\n",
    "# Activate your environment (correct path)\n",
    "conda activate /u/hsekar/comics_env\n",
    "\n",
    "# Verify environment\n",
    "echo \"Python: $(which python)\"\n",
    "echo \"Conda env: $CONDA_PREFIX\"\n",
    "\n",
    "cd /scratch/bftl/hsekar/comics_project\n",
    "python scripts/zero_shot_eval.py\n",
    "\n",
    "echo \"Job finished: $(date)\"\n",
    "'''\n",
    "\n",
    "sbatch_path = Path(\"/scratch/bftl/hsekar/comics_project/scripts/zero_shot_eval.sbatch\")\n",
    "with open(sbatch_path, 'w') as f:\n",
    "    f.write(sbatch_content)\n",
    "\n",
    "print(f\"SLURM script saved to: {sbatch_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0acfa75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 54530\n",
      "\n",
      "First sequence type: <class 'dict'>\n",
      "\n",
      "First sequence keys: dict_keys(['comic_no', 'story_idx', 'context', 'target', 'target_text', 'context_texts'])\n",
      "\n",
      "First sequence content:\n",
      "{'comic_no': np.int64(1), 'story_idx': 10, 'context': [{'comic_no': 1, 'page_no': 47, 'panel_no': 0, 'image_path': '/scratch/bftl/hsekar/comics_project/data/images/1/47_0.jpg', 'text': ''}, {'comic_no': 1, 'page_no': 47, 'panel_no': 1, 'image_path': '/scratch/bftl/hsekar/comics_project/data/images/1/47_1.jpg', 'text': 'IN 1962 WITH A ROCKET LAND ING ON THE MOON. IN 1977, MAN SET FOOT ON MARS. A CENTURY LATER ON ALPHA CENTAURI, THE NEAREST STAF BY 3750, MANY STAR CLUSTERS HAD BEEN EXPLORED, THEIR PLANETARY SYSTEMS JOINED WITH EARTH FEDERATION. TO POLICE THIS VAST AREA OF BILLIONS OF MILES OF EMPTY SPACE-TO GUARD THE TREAS- URE-LADEN CARGO SPACERS, THE STAR PATROL WAS BORN. DAVE KENTON WAS A STAR PATROL MAN, HIS HAND WAS ADEPT WITH SWORD AND GUN-HE WAS READY TO DIE IN ORDER TO SMASH THE POWER OF..... THE CORSAIRS FROM THE COALSACK!'}, {'comic_no': 1, 'page_no': 47, 'panel_no': 2, 'image_path': '/scratch/bftl/hsekar/comics_project/data/images/1/47_2.jpg', 'text': \"SACK SINCE 1950, HAVE BEEN RAIDINS THE TREASURE-HEAVY SPACERS... HEAVE OVER, BOYS! WE'RE ALMOST ABOVE HER!\"}, {'comic_no': 1, 'page_no': 47, 'panel_no': 3, 'image_path': '/scratch/bftl/hsekar/comics_project/data/images/1/47_3.jpg', 'text': \"THE SCREAMS AND MOANS OF THEIR VICTIMS SOUNDED FOR A TIME ABOVE THE WHIRR OF THE PIRATES' BEAM-GUNS- AND THEN SILENCE FELL, AND THE LOOTING BEGAN... SURRENDER NOW- AND YOU LIVE! FIGHT- AND DIE!\"}, {'comic_no': 1, 'page_no': 48, 'panel_no': 0, 'image_path': '/scratch/bftl/hsekar/comics_project/data/images/1/48_0.jpg', 'text': 'ON THE TINY PLANET OF FLAYAL-HUNDREDS OF LIGHT YEARS FROM THE EARTH-YOUNG STAR PATROLMAN DAVE KENTON RECEIVES WORD OF THE SPACE DISASTER... THE COALSACK PIRATES-AGAIN! GOT THE ANNUAL METAL RUN FROM DEBEB! THREE BILLIONS CREDITS WORTH OF GOLD AND PLATINUM- GONE!'}], 'target': {'comic_no': 1, 'page_no': 48, 'panel_no': 1, 'image_path': '/scratch/bftl/hsekar/comics_project/data/images/1/48_1.jpg', 'text': \"JUST GOT MY ORDERS, SWEETHEART! I'M ROCKET- TING UP TONIGHT! SEALED ORDERS-TO GET THOSE PIRATES! ONE MAN- AGAINST A FLEET OF SPACE ROVERS?\"}, 'target_text': \"JUST GOT MY ORDERS, SWEETHEART! I'M ROCKET- TING UP TONIGHT! SEALED ORDERS-TO GET THOSE PIRATES! ONE MAN- AGAINST A FLEET OF SPACE ROVERS?\", 'context_texts': ['', 'IN 1962 WITH A ROCKET LAND ING ON THE MOON. IN 1977, MAN SET FOOT ON MARS. A CENTURY LATER ON ALPHA CENTAURI, THE NEAREST STAF BY 3750, MANY STAR CLUSTERS HAD BEEN EXPLORED, THEIR PLANETARY SYSTEMS JOINED WITH EARTH FEDERATION. TO POLICE THIS VAST AREA OF BILLIONS OF MILES OF EMPTY SPACE-TO GUARD THE TREAS- URE-LADEN CARGO SPACERS, THE STAR PATROL WAS BORN. DAVE KENTON WAS A STAR PATROL MAN, HIS HAND WAS ADEPT WITH SWORD AND GUN-HE WAS READY TO DIE IN ORDER TO SMASH THE POWER OF..... THE CORSAIRS FROM THE COALSACK!', \"SACK SINCE 1950, HAVE BEEN RAIDINS THE TREASURE-HEAVY SPACERS... HEAVE OVER, BOYS! WE'RE ALMOST ABOVE HER!\", \"THE SCREAMS AND MOANS OF THEIR VICTIMS SOUNDED FOR A TIME ABOVE THE WHIRR OF THE PIRATES' BEAM-GUNS- AND THEN SILENCE FELL, AND THE LOOTING BEGAN... SURRENDER NOW- AND YOU LIVE! FIGHT- AND DIE!\", 'ON THE TINY PLANET OF FLAYAL-HUNDREDS OF LIGHT YEARS FROM THE EARTH-YOUNG STAR PATROLMAN DAVE KENTON RECEIVES WORD OF THE SPACE DISASTER... THE COALSACK PIRATES-AGAIN! GOT THE ANNUAL METAL RUN FROM DEBEB! THREE BILLIONS CREDITS WORTH OF GOLD AND PLATINUM- GONE!']}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL: Check sequence structure\n",
    "\"\"\"\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "PROCESSED_DIR = Path(\"/scratch/bftl/hsekar/comics_project/data/processed\")\n",
    "\n",
    "# Load one sequence\n",
    "with open(PROCESSED_DIR / \"test_sequences.pkl\", \"rb\") as f:\n",
    "    test_sequences = pickle.load(f)\n",
    "\n",
    "# Print the structure of the first sequence\n",
    "print(\"Number of sequences:\", len(test_sequences))\n",
    "print(\"\\nFirst sequence type:\", type(test_sequences[0]))\n",
    "print(\"\\nFirst sequence keys:\", test_sequences[0].keys() if isinstance(test_sequences[0], dict) else \"Not a dict\")\n",
    "print(\"\\nFirst sequence content:\")\n",
    "print(test_sequences[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
